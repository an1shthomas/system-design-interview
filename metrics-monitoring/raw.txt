Please summarize this web page




2 steps
2 steps
This is a system design interview guide from Hello Interview covering how to design a metrics monitoring platform similar to Datadog. Here's a summary: Problem Overview The system must collect performance metrics (CPU, memory, latency, etc.) from 500,000 servers, store them as time-series data, serve dashboard queries, and fire alerts when thresholds are breached. The core challenge is ingesting 5 million metrics per second at ~1 GB/s of raw data. Core Requirements The functional goals are: ingest metrics, support dashboard queries with filters/aggregations, define alert rules with thresholds, and send notifications (Slack, PagerDuty, email). The non-functional goals are sub-second dashboard queries, alerts firing within 1 minute, and high availability. High-Level Architecture (4 main flows)

Ingestion: Rather than servers posting directly to a central service, the recommended approach uses agent-based collection â€” a lightweight daemon runs on each server, batches metrics locally, then flushes to a Kafka queue. This reduces the central load from 5M req/s to ~50K req/s and provides a durable buffer against downstream failures.
Storage & Querying: A dedicated time-series database (e.g., InfluxDB, VictoriaMetrics) is used instead of a relational DB, because it's optimized for append-only writes, time-based partitioning, and columnar compression. A separate query service translates a PromQL-like DSL into storage queries.
Alerting: Alert rules are stored in Postgres and evaluated via a polling-based Alert Evaluator that periodically queries the time-series DB â€” the same approach used by Prometheus Alertmanager.
Notifications: A dedicated Notification Service sits between the alert evaluator and external channels, handling deduplication (don't page every minute for the same ongoing issue), grouping (don't send 100 pages for 100 servers in one cluster), silencing, and escalation. Key Deep Dives
Low-latency dashboard queries: Use pre-computed rollups at multiple resolutions (raw â†’ 1-min â†’ 1-hour â†’ 1-day) so long-range queries don't scan billions of raw data points. Add a Redis caching layer with query splitting for near-instant results.
Faster alerting (sub-minute): Stream processing with Apache Flink reading directly from Kafka can evaluate alert conditions on in-memory windowed state, reducing latency from ~60 seconds to seconds.
High availability: Use redundancy at every layer (load-balanced ingestion, replicated Kafka, multi-node TSDB), local agent buffering for retries, and idempotent writes to avoid data loss during failures.
Cardinality explosion: A metric like http_requests with many label dimensions can produce millions of unique time series, degrading write and read performance. The solution is a policy store (in Postgres) defining allowed labels/caps, and a Redis-based cardinality tracker at the ingestion layer that drops metrics exceeding their series cap. Interview Expectations by Level Mid-level candidates should identify the need for a message queue and time-series storage. Senior candidates should proactively raise cardinality concerns and rollup strategies. Staff+ candidates should discuss operational challenges like meta-monitoring (monitoring the monitoring system), schema migration, and multi-tenancy.
Can you act as a mentor and help me with this system design problem?



I'd love to help you work through this! No browser tools needed for this â€” let's just talk it through. To get started, let me play the role of interviewer/mentor. I'll ask you questions, give you hints when you're stuck, and provide feedback on your reasoning. Here's the problem prompt:

Design a metrics monitoring platform like Datadog. The system should collect performance metrics (CPU, memory, latency, custom counters) from a large fleet of servers, store them, allow users to query and visualize them on dashboards, and send alerts when thresholds are breached.

Before I start probing, a couple of quick questions to calibrate:

What's your experience level? (e.g., mid-level, senior, staff) â€” so I know how deep to push.
What would you like to focus on? We can go through the whole problem end-to-end, or zoom in on a specific area (e.g., ingestion at scale, storage choices, alerting, cardinality). Once I know where you're starting from, we'll dive in. Take your time â€” think out loud as you would in a real interview!
experience - staff, end-to-end



Perfect. At the staff level, I'll expect you to drive the conversation, proactively surface trade-offs, and have opinions backed by reasoning. I'll push back and probe when I want more depth. Let's begin. I'll play the interviewer.
"Design a metrics monitoring platform like Datadog." The system needs to:

Collect performance metrics (CPU, memory, latency, custom counters) from servers and services
Store and allow querying of that data
Support dashboards with filters, aggregations, and time ranges
Trigger alerts and send notifications when thresholds are breached You have ~45 minutes. The floor is yours â€” go ahead and start wherever you think is right. (Take your time and respond as you would in a real interview. I'll ask follow-up questions as we go.)
I would like to get a guidance. Can you act both as an interviewer and interview (staff engineer) and go step by step. Please ask me to before you go to each step.



Got it! I'll act as both roles â€” I'll show you what a strong staff-level candidate would say at each step, and also what the interviewer would ask/probe. We'll go step by step, and I'll pause and wait for your "go ahead" before moving to the next step.
Here's the structure we'll follow:

Clarifying Requirements (Functional & Non-Functional)
Core Entities & Data Modeling
Data Flow / High-Level Architecture
API Design
Deep Dives (Storage, Ingestion at scale, Alerting, Query performance, Cardinality, HA)
Wrap-up & Trade-offs
Ready to start with Step 1: Clarifying Requirements? Just say "go ahead" (or ask any questions you have first) and I'll walk through how a staff engineer would open this interview.

go ahead



Step 1: Clarifying Requirements
ğŸ¤ Interviewer:
"Design a metrics monitoring platform like Datadog. Where would you like to start?"
ğŸ‘¨â€ğŸ’» Staff Engineer (candidate):
"Before jumping into the design, I'd like to ask a few clarifying questions to make sure I'm solving the right problem." Functional scope:

"Are we covering the full observability stack â€” metrics, logs, and traces â€” or just metrics for now?" (Interviewer says: just metrics)
"Should we support both infrastructure metrics like CPU/memory and custom application metrics like request counts or business counters?" (Yes, both)
"For dashboards, are we building the visualization layer itself, or just the query/data API that a frontend would consume?" (Just the backend â€” API + storage)
"For alerting, what notification channels do we need? Slack, PagerDuty, email?" (Yes, all three) Scale:
"How large is the fleet we're monitoring? I want to size the ingestion problem correctly." (Interviewer says: 500,000 servers)
"How frequently do servers emit metrics?" (Every 10 seconds)
"How long do we need to retain data?" (Let's say 1 year) Consistency & latency:
"How quickly do alerts need to fire after a threshold is breached â€” seconds, or is sub-minute acceptable?" (Sub-minute is fine)
"For dashboards, are stale reads okay, or do we need real-time freshness?" (Eventual consistency is fine for dashboards)
ğŸ“ Back-of-the-envelope (candidate does this out loud):
"Let me quickly size the problem so our design decisions are grounded in numbers."

500,000 servers Ã— 100 metrics each Ã— 1 emit per 10s = ~5 million metrics/second
Each data point â‰ˆ 150 bytes (timestamp + value + labels) â†’ ~750 MB/s raw ingestion
Per day: ~65 TB of raw data. With compression (time-series DBs compress 10â€“20x), that's ~3â€“6 TB/day stored
Per year: ~1â€“2 PB, but with rollups and tiered retention, manageable "This is clearly a write-heavy system with a massive ingestion challenge at its core. The read pattern is bursty â€” engineers query during incidents. These two patterns need to be designed independently."
ğŸ¤ Interviewer follow-up:
"Good. What would you say are the most important non-functional requirements given that scale?"

ğŸ‘¨â€ğŸ’» Candidate:
High write throughput â€” 5M metrics/sec, must not drop data
Low-latency reads â€” dashboard queries should return in seconds even over weeks of data
Alert reliability â€” alerts must fire even if parts of the system are degraded
High availability â€” the monitoring system is most critical exactly when things are going wrong
Cardinality control â€” unbounded label combinations can silently kill the system
âœ… Key insight a staff engineer shows here:
They don't just list requirements mechanically. They connect the scale math to design consequences ("write-heavy and read-bursty means we separate these paths") and proactively name cardinality as a non-obvious but critical constraint.
Ready for Step 2: Core Entities & Data Modeling? Just say "go ahead"!

Please check again, it looks like the math is not correct:
500,000 servers Ã— 100 metrics each Ã— 1 emit per 10s = ~5 million metrics/second



Great catch! Let's redo that math:

500,000 servers Ã— 100 metrics each = 50,000,000 metrics every 10 seconds
50,000,000 Ã· 10 seconds = 5,000,000 metrics/second Actually, the result of 5 million metrics/second is correct â€” the arithmetic just needs to be read carefully:
500K servers Ã— 100 metrics = 50M metrics per 10s batch â†’ 50M / 10 = 5M metrics/second âœ… The way it was written was a bit ambiguous â€” "1 emit per 10s" means each server emits 100 metrics once every 10 seconds as a batch, not 1 metric every 10 seconds. So the math holds, but the phrasing could be clearer. A better way to state it: "500,000 servers each emit 100 metrics every 10 seconds. That's 50 million metrics per 10-second window, or 5 million metrics/second." Good instinct to question it â€” in a real interview, showing that kind of rigor with numbers is exactly what interviewers look for at the staff level. Ready to continue to Step 2: Core Entities & Data Modeling? Just say "go ahead"!

how did you come up with Each data point â‰ˆ 150 bytes



Great question! Here's the breakdown of how a staff engineer would reason through it out loud:
A single metric data point typically contains:

Metric name â€” e.g., cpu_usage â†’ ~10â€“20 bytes
Timestamp â€” Unix epoch (int64) â†’ 8 bytes
Value â€” a float64 â†’ 8 bytes
Labels/tags â€” this is the biggest variable. A typical set might be host="server-1", region="us-east", service="api". Each key-value pair is ~20â€“30 bytes, and you might have 3â€“5 labels â†’ ~60â€“120 bytes Adding it up: ~20 + 8 + 8 + 100 (labels, middle estimate) = ~136 bytes, so ~150 bytes is a reasonable round number.
âš ï¸ Important nuance a staff engineer would add:
"The label size is actually the most important variable here and it's workload-dependent. If teams use verbose labels or many of them, a data point could easily be 300â€“500 bytes. If labels are minimal, it could be closer to 50â€“80 bytes. For the purposes of this estimate I'll use 150 bytes as a reasonable middle ground, but I'd flag this as a key assumption."
In a real interview, you don't need to be exact â€” you need to show structured reasoning from first principles and acknowledge your assumptions. That's what separates a staff answer from a mid-level one. Shall we move on to Step 2: Core Entities & Data Modeling? Just say "go ahead"!

so 1 byte is 1 char ?



Almost! Here's the precise answer: In ASCII encoding (which covers standard English letters, digits, and common symbols): yes, 1 character = 1 byte. Since metric names, label keys, and label values are typically plain ASCII strings (e.g., cpu_usage, us-east, server-1), the 1 byte per character assumption is valid and commonly used in back-of-the-envelope calculations. So for example:

cpu_usage â†’ 9 characters â†’ 9 bytes
host="server-123" â†’ 16 characters â†’ ~16 bytes
region="us-east" â†’ 16 characters â†’ ~16 bytes However, if you were dealing with Unicode (e.g., multi-language characters), it's a different story â€” UTF-8 can use 1â€“4 bytes per character. But for system design interviews with typical infrastructure metrics, the 1 char â‰ˆ 1 byte assumption is perfectly reasonable and expected.
This is actually a good habit to have in interviews â€” when you say "metric name is ~20 bytes", you can justify it by saying "metric names are ASCII strings, typically 10â€“20 characters, so roughly 10â€“20 bytes." It shows your estimate isn't pulled from thin air. Ready for Step 2: Core Entities & Data Modeling? Just say "go ahead"!

go ahead



Step 2: Core Entities & Data Modeling
ğŸ¤ Interviewer:
"Before we get into the architecture, can you walk me through the core entities in this system and how they relate to each other?"
ğŸ‘¨â€ğŸ’» Staff Engineer (candidate):
"Sure. Let me identify the key 'nouns' in the system first, because getting this right avoids a lot of confusion later."
1. Label A key-value pair attached to a metric to give it context.

e.g., host="server-1", region="us-east", service="payments"
Labels are how users slice and filter data on dashboards 2. Metric A named measurement with a value at a point in time, decorated with labels.
e.g., cpu_usage{host="server-1", region="us-east"} = 0.73 at t=1640000000
The name describes what is being measured; labels describe where/who 3. Series The full sequence of (timestamp, value) pairs for one unique metric name + label set combination over time.
cpu_usage{host="server-1"} over 30 days = one series
cpu_usage{host="server-2"} = a different series
500K servers each reporting cpu_usage = 500K series for that one metric alone
"This is the crux of the data model. A 'series' is the atomic unit of storage, and the number of series â€” called cardinality â€” is the primary scaling challenge. I'll come back to this in the deep dive." 4. Data Point A single (timestamp, value) entry within a series. This is what gets written at high volume â€” 5M/sec in our case. 5. Alert Rule A user-defined condition that is evaluated periodically. It has:

A metric query (e.g., avg(cpu_usage{region="us-east"}))
A threshold (e.g., > 0.9)
A duration (e.g., for 5 minutes)
Notification channels (e.g., Slack, PagerDuty) 6. Dashboard A collection of panels, each backed by a metric query. Drives the read path â€” queries are bursty and span potentially large time ranges.
ğŸ“ Key relationship to highlight:
Label (k=v pairs)
    â†“
Metric (name + labels + value + timestamp)
    â†“
Series (unique metric name + label combination â†’ sequence of data points)
    â†“
Data Point (timestamp, value) â€” written at 5M/sec
ğŸ¤ Interviewer follow-up:
"You mentioned cardinality. Can you give me a quick example of how it can explode?"

ğŸ‘¨â€ğŸ’» Candidate:
"Sure. Take a metric like http_requests. If I add labels for host (500K values), endpoint (200 values), status_code (10 values), and method (5 values) â€” the theoretical maximum number of unique series is: 500,000 Ã— 200 Ã— 10 Ã— 5 = 5 billion series In practice, not every combination exists, but you can easily hit tens of millions of series. Each series has overhead in the storage engine â€” its own index entry, in-memory tracking, and write buffer. At that scale, the database starts to degrade silently: write throughput drops, memory spikes, and queries slow down. This is why cardinality is a first-class design concern, not an afterthought."
âœ… What makes this staff-level:
Connects entities to the scaling problem (series â†’ cardinality)
Doesn't just list entities but explains why each one matters to the design
Proactively foreshadows a deep dive topic without getting lost in it yet
Ready for Step 3: Data Flow & High-Level Architecture? Just say "go ahead"!

go ahead



Step 3: Data Flow & High-Level Architecture
ğŸ¤ Interviewer:
"Walk me through the high-level architecture. How does data flow through the system end to end?"
ğŸ‘¨â€ğŸ’» Staff Engineer (candidate):
"Let me trace the data flow first, then justify each component choice. I'll split the system into three paths: the write path, the read path, and the alert path â€” because they have fundamentally different characteristics and need to be designed independently."
âœï¸ Write Path (Ingestion)
Naive approach first: "The simplest thing is to have each server POST metrics directly to an ingestion service, which writes to a database. This works at small scale but breaks at 5M metrics/sec for two reasons: the ingestion service becomes a bottleneck, and the database gets hammered directly with no buffer." Better approach â€” what I'd actually propose:

Agent â†’ Kafka â†’ Ingestion Consumers â†’ Time-Series DB

Agent (on each server): A lightweight background process (like the Datadog Agent or OpenTelemetry Collector) runs on every server. It:
Collects metrics locally at high frequency
Batches them (e.g., every 10 seconds)
Flushes to Kafka via HTTP or gRPC "This is critical. Instead of 5M individual writes/sec hitting a central service, agents batch locally and send ~50K batched requests/sec. We've reduced central load by 100x at the edge."
Kafka: Acts as a durable, high-throughput buffer between producers and consumers. Partitioned by a hash of metric name + labels to ensure the same series always goes to the same partition (important for ordering). "Kafka gives us backpressure handling, durability, and replay capability. If the storage layer is slow or temporarily down, Kafka absorbs the spike. We can also replay data after a consumer failure â€” critical for a monitoring system."
Ingestion Consumers: A fleet of consumers reads from Kafka, validates data points (schema, label allowlists), enforces cardinality limits, and writes batches to the time-series DB.
Time-Series DB (e.g., InfluxDB, VictoriaMetrics, TimescaleDB): Purpose-built for this workload. Optimized for append-only writes, time-based partitioning, and columnar compression.
ğŸ“– Read Path (Dashboards)
User â†’ Query Service â†’ Cache (Redis) â†’ Time-Series DB

Query Service: Accepts queries in a PromQL-like DSL, translates them to storage queries, selects appropriate data resolution (raw vs. rollups), and returns formatted results.
Redis Cache: Caches query results. Since most dashboard queries overlap heavily (e.g., a 24-hour window queried every 30 seconds shifts by only 30 seconds), cache hit rates are very high.
Time-Series DB: Serves queries against pre-computed rollups for long time ranges, raw data for recent windows. "The read and write paths share the same storage layer but are completely decoupled in terms of services. This lets us scale and tune them independently â€” dashboard query spikes during incidents don't interfere with ingestion."
ğŸ”” Alert Path
Alert Evaluator â†’ (queries Time-Series DB on schedule) â†’ Notification Service â†’ Slack/PagerDuty/Email

Alert Evaluator: Periodically (every ~30â€“60s) fetches active alert rules from a config store (Postgres), executes the corresponding metric query against the Time-Series DB, and checks if the threshold condition is met for the required duration.
Notification Service: Handles deduplication (don't re-page for the same ongoing alert), grouping (batch 100 server alerts into one incident), silencing, and escalation. Writes alert events to Kafka before dispatching to external channels â€” so if Slack is down, we don't lose the alert.
ğŸ—ºï¸ Full Picture (described verbally for the whiteboard):
[Servers]
    â†“ (agent batching)
[Kafka]
    â†“ (consumers)              â†â†’ [Cardinality Enforcer]
[Ingestion Service]
    â†“
[Time-Series DB] â†â†’ [Rollup Worker]
    â†‘
[Query Service] â†â†’ [Redis Cache]
    â†‘
[Dashboard / Users]
[Alert Rules DB (Postgres)]
    â†“
[Alert Evaluator] â†’ polls Time-Series DB
    â†“
[Notification Service] â†’ Kafka â†’ Slack / PagerDuty / Email
ğŸ¤ Interviewer follow-up:
"Why Kafka specifically? Couldn't you just use a load-balanced ingestion service with auto-scaling?"

ğŸ‘¨â€ğŸ’» Candidate:
"Auto-scaling helps the ingestion service itself but just moves the bottleneck downstream to the database â€” you now have 50 instances all hammering storage simultaneously with no buffer. The real problem is that writes and storage are tightly coupled. Kafka decouples them. It also gives us durability â€” if the DB slows down for 2 minutes, Kafka holds those metrics and consumers catch up. Without it, you're just dropping data. And for a monitoring system, dropping data during an incident â€” exactly when you need it most â€” is unacceptable."
ğŸ¤ Interviewer follow-up:
"What's the trade-off of using agents vs. servers pushing directly?"

ğŸ‘¨â€ğŸ’» Candidate:
"The main trade-off is operational complexity. You now have a daemon to deploy, version, configure, and monitor on 500K servers. That's non-trivial. But the benefits outweigh it: you get local buffering (metrics survive brief network outages), local aggregation (compute percentiles at the edge before shipping), and a massive reduction in central ingestion load. Every production-grade monitoring system â€” Datadog, Prometheus, OTEL â€” uses this pattern for exactly these reasons."
âœ… What makes this staff-level:
Starts with the naive approach and explains why it fails before proposing the real solution
Clearly separates write, read, and alert paths with different design rationale for each
Justifies every component with trade-offs, not just "I'd use Kafka because it's good"
Connects decisions back to the scale numbers established in Step 1
Ready for Step 4: API Design? Just say "go ahead"!

go ahead



Step 4: API Design
ğŸ¤ Interviewer:
"Let's define the API. What interfaces does this system expose and to whom?"
ğŸ‘¨â€ğŸ’» Staff Engineer (candidate):
"There are three distinct APIs here serving different clients with very different usage patterns. Let me walk through each one."
1. ğŸ“¥ Ingestion API
Client: Agents running on servers (machine-to-machine, extremely high volume)

POST /v1/metrics/ingest
Content-Type: application/x-protobuf   â† not JSON, I'll explain why
{
  "metrics": [
    {
      "name": "cpu_usage",
      "labels": { "host": "server-1", "region": "us-east" },
      "value": 0.73,
      "timestamp": 1640000000
    },
    {
      "name": "memory_used_bytes",
      "labels": { "host": "server-1", "region": "us-east" },
      "value": 4294967296,
      "timestamp": 1640000000
    }
  ]
}
Response: 202 Accepted
Key design decisions to call out:

Batched, not one-at-a-time: Each request contains many data points. This amortizes HTTP overhead across many metrics and is why agents batch locally before flushing.
Protobuf over JSON: At 50K requests/sec from agents, JSON parsing overhead is real. Protobuf is ~3â€“10x smaller on the wire and faster to serialize/deserialize. I'd mention this to the interviewer and note I'm using JSON here for readability.
202 Accepted, not 200 OK: The ingestion service publishes to Kafka and returns immediately â€” it doesn't wait for the DB write. Async acknowledgment is intentional.
Timestamps from the agent, not the server: Agents include the timestamp of when the metric was observed, not when it arrives at the ingestion service. This handles late/out-of-order data correctly.
2. ğŸ“Š Query API
Client: Dashboard frontend, on-call engineers (human-driven, bursty, read-heavy)

GET /v1/metrics/query
  ?query=avg(cpu_usage{region="us-east"})
  &start=1640000000
  &end=1640086400
  &step=60
Response: 200 OK
{
  "metric": "cpu_usage",
  "labels": { "region": "us-east" },
  "datapoints": [
    { "timestamp": 1640000000, "value": 0.71 },
    { "timestamp": 1640000060, "value": 0.74 },
    ...
  ]
}
Key design decisions:

DSL-based query (query param): Using a PromQL-inspired DSL rather than raw SQL. This allows the query service to optimize execution â€” selecting the right rollup resolution, pushing down filters to storage, etc.
step parameter: Controls the granularity of the returned data. A 30-day query with step=3600 returns hourly aggregates, not raw 10-second points. The query service uses this to select the appropriate rollup tier.
GET, not POST: Queries are idempotent and cacheable. GET requests with query params can be cached by the Redis layer using the full URL as the cache key.
3. ğŸ”” Alert Rules API
Client: Engineers configuring monitoring (low volume, write-once-read-many)

POST /v1/alerts/rules
{
  "name": "High CPU - US East",
  "query": "avg(cpu_usage{region='us-east'}) > 0.9",
  "for": "5m",
  "severity": "critical",
  "notifications": [
    "slack:#oncall-infra",
    "pagerduty:team-platform"
  ]
}
Response: 201 Created
{ "rule_id": "rule_abc123" }
GET    /v1/alerts/rules              â†’ list all rules
GET    /v1/alerts/rules/{rule_id}    â†’ get one rule
PUT    /v1/alerts/rules/{rule_id}    â†’ update a rule
DELETE /v1/alerts/rules/{rule_id}    â†’ delete a rule
GET    /v1/alerts/active             â†’ list currently firing alerts
Key design decisions:

for duration field: Prevents flapping. An alert only fires if the condition is continuously breached for the specified duration. This is critical in practice â€” a CPU spike for 5 seconds shouldn't page anyone.
severity field: Allows the Notification Service to route differently â€” critical alerts go to PagerDuty immediately, warnings go to Slack.
Separation of rule definition from alert state: POST /alerts/rules defines the condition. GET /alerts/active shows what's currently firing. These are different concepts stored separately.
ğŸ¤ Interviewer follow-up:
"You mentioned protobuf for ingestion. What about the query API â€” any concerns there?"

ğŸ‘¨â€ğŸ’» Candidate:
"For the query API, JSON is fine. The volume is much lower â€” this is human-driven, maybe thousands of requests per minute at peak, not millions per second. The readability and debuggability of JSON is more valuable there than the wire efficiency of protobuf. I'd only reach for protobuf on the hot path where volume justifies the added complexity."
ğŸ¤ Interviewer follow-up:
"How would you handle authentication and rate limiting on the ingestion API?"

ğŸ‘¨â€ğŸ’» Candidate:
"Each agent would authenticate with an API key scoped to a tenant or team â€” passed as a header. An API gateway sitting in front of the ingestion service handles auth validation and rate limiting per API key. Rate limits serve two purposes: protecting the system from runaway agents, and enforcing fair usage in a multi-tenant setup. I'd put this in the API gateway rather than the ingestion service itself to keep concerns separated â€” the ingestion service should be focused purely on throughput."
âœ… What makes this staff-level:
Justifies every design decision â€” protobuf vs. JSON, 202 vs. 200, GET vs. POST
Thinks about who the client is and designs the API accordingly (agent vs. human)
Proactively raises auth, rate limiting, and caching as first-class concerns
Connects API design back to the system's scaling constraints
Ready for Step 5: Deep Dives? This is where the staff-level interview really happens. We'll cover:

Storage & rollups
Ingestion at scale
Low-latency alerting
High availability
Cardinality explosion Just say "go ahead"!




Step 5: Deep Dives

"This is where staff-level interviews are won or lost. The high-level design is table stakes â€” the deep dives show your depth of expertise and engineering judgment. I'll go through each one. Let's start with the most fundamental: storage."
ğŸ—„ï¸ Deep Dive 1: Storage & Query Performance

ğŸ¤ Interviewer:
"You mentioned a time-series database. Why not just use Postgres? Walk me through the storage design."
ğŸ‘¨â€ğŸ’» Candidate:
Why not Postgres?
"Let me walk through why Postgres breaks at our scale before explaining what we'd use instead."

At 5M writes/sec, Postgres's B-tree indexes become a bottleneck â€” every insert requires updating the index, and B-trees don't handle append-heavy workloads well
Queries like "give me avg CPU over 30 days for 1,000 servers" require scanning billions of rows â€” even with indexes, this is painfully slow
Retention management (deleting old data) causes table bloat, autovacuum pressure, and write amplification
Storing time-series data row-by-row wastes space â€” timestamps and values for one series compress dramatically when stored together
Why a Time-Series DB?
Time-series databases are built around three key properties of metrics data:


Append-only writes: Metrics are almost never updated, only added. TSDBs use LSM-tree (Log-Structured Merge-tree) storage engines which are optimized for sequential appends â€” no random I/O, no index update overhead.
Time-based partitioning: Data is chunked by time (e.g., hourly or daily blocks). A query for "last 6 hours" only touches 6 blocks. Dropping old data is as simple as deleting a block â€” no vacuum, no fragmentation.
Columnar compression: Within a time block, timestamps and values for one series are stored together. Timestamps compress with delta encoding (store differences, not absolutes â€” e.g., 10, 10, 10, 10 instead of 1000, 1010, 1020, 1030). Values compress with XOR encoding (Gorilla compression). A day of CPU metrics that would be 100KB uncompressed compresses to ~5KB â€” a 20x reduction.
Rollup Strategy:
"Raw data at 10-second resolution is only useful for short windows. For longer queries we need pre-aggregated rollups. I'd propose a tiered retention policy:"
| Resolution | Retention | Use case |
|---|---|---|
| Raw (10s) | 2 days | Debugging recent incidents |
| 1-minute rollup | 2 weeks | Short-term trend analysis |
| 1-hour rollup | 90 days | Medium-term capacity planning |
| 1-day rollup | 2 years | Long-term trend / compliance |
"A background Rollup Worker continuously reads raw data and computes aggregates (min, max, avg, count, sum â€” and importantly, histograms for percentiles) writing them to separate rollup tables. The Query Service selects the appropriate tier based on the requested time range and step."
Sharding:
"We shard the TSDB by a hash of metric name + label set. This ensures all data points for one series land on the same shard, which is critical for time-range queries â€” you never need to fan out across shards for a single series. For cross-series aggregations (e.g., avg across all hosts), we fan out in parallel and merge at the query layer."


ğŸ¤ Interviewer:
"What about percentile metrics like p99 latency? Can you compute those from rollups?"
ğŸ‘¨â€ğŸ’» Candidate:
"This is a subtle but important point. You cannot accurately compute percentiles from pre-aggregated averages â€” taking the average of averages loses the distribution. The right approach is to store histograms at each rollup level. Each data point for a latency metric is bucketed (e.g., 0-10ms, 10-50ms, 50-100ms, 100-500ms, 500ms+), and the histogram is what gets rolled up. From a histogram you can estimate any percentile at query time. This is exactly how Prometheus histograms and Datadog distributions work."
ğŸ¤ Interviewer:
"How do you handle low-latency dashboard queries over weeks of data?"
ğŸ‘¨â€ğŸ’» Candidate:
"Three layers working together:"

Rollups (as above) â€” the right resolution means fewer data points to scan
Redis caching â€” query results are cached with a key of hash(query + time range). Since dashboard panels auto-refresh every 30 seconds, and the query window shifts by only 30 seconds, we split queries into a cached historical chunk and a fresh recent chunk. The historical part is a cache hit; only the last few minutes hit the DB.
Pre-computation for popular dashboards â€” a background job identifies frequently-executed queries and pre-computes their results on a schedule, warming the cache before users ask
"With this combination, most dashboard queries are served from Redis in under 50ms. Only the first load of a cold query hits the time-series DB."


Ready for Deep Dive 2: Alerting & Reducing Alert Latency?
Just say "go ahead" and we'll cover:

Why polling is good enough for most cases
When and how to use stream processing (Flink) for sub-second alerting
Alert flapping, deduplication, and the Notification Service design

Deep Dive 2: Alerting & Notifications

ğŸ¤ Interviewer:
"Walk me through the alerting system in detail. How do alerts get evaluated and how do notifications reach the right people reliably?"
ğŸ‘¨â€ğŸ’» Candidate:
"I'll split this into two parts: alert evaluation (detecting the condition) and notification delivery (getting it to the right person reliably). They're separate concerns with different failure modes."
Part A: Alert Evaluation
The polling approach (baseline):
"Since our SLA is sub-minute alerting, a simple polling-based evaluator works well and is battle-tested â€” it's exactly how Prometheus Alertmanager works."
The Alert Evaluator:

Fetches all active alert rules from Postgres (cached in-memory, refreshed every ~30s)
For each rule, executes the metric query against the Time-Series DB
Checks if the condition is breached continuously for the for duration
If yes, emits an alert event to Kafka
"At 10,000 alert rules evaluated every 30 seconds, that's ~333 queries/second to the TSDB. That's very manageable â€” these are simple point queries, not the expensive multi-week dashboard scans. I'd run multiple Alert Evaluator instances, each owning a shard of alert rules, to parallelize evaluation and avoid a single point of failure."


ğŸ¤ Interviewer:
"What if someone needs alerts in under 5 seconds? How would you approach that?"
ğŸ‘¨â€ğŸ’» Candidate:
Stream processing approach (for real-time alerting):
"Polling fundamentally can't go below its interval. If we need sub-10-second alerting, we need to evaluate conditions as data arrives, not on a schedule. The answer is stream processing with Flink."
Architecture change:

Flink runs as a second consumer on the same Kafka topic as the ingestion consumers
For each metric series, Flink maintains windowed state in memory â€” e.g., a 5-minute sliding window of values
Alert rules are compiled into Flink operators that continuously evaluate conditions against those windows
When a threshold is violated for the configured duration, Flink emits an alert event immediately â€” no DB query needed
"The key insight is that Flink evaluates against in-memory state, not the database. Latency drops from 'up to 60 seconds' to 'within seconds of the metric arriving in Kafka' â€” typically 2-5 seconds including network and processing lag."
Trade-offs to call out proactively:
Operational complexity: Flink clusters need checkpointing, state management, and careful failure recovery. If a Flink node crashes mid-window, you need to restore its window state from the last checkpoint.
Rule changes are hard: When a user updates an alert rule, you need to update the corresponding Flink operator without losing in-flight window state. This is non-trivial.
Memory pressure: Maintaining windowed state for millions of series across thousands of rules can be memory-intensive. You need careful capacity planning.
"My recommendation: use polling for the vast majority of alerts â€” it's simpler, cheaper, and sub-minute is fine for most scenarios. Reserve Flink for a 'real-time alerts' tier that users explicitly opt into, knowing it costs more to run. This is actually how Datadog structures it â€” most alerts are evaluated periodically, with a premium tier for streaming evaluation."


Part B: Notification Delivery
ğŸ¤ Interviewer:
"How do you make sure notifications are reliable and don't overwhelm on-call engineers?"
ğŸ‘¨â€ğŸ’» Candidate:
"Having the Alert Evaluator call Slack or PagerDuty directly is tempting but fragile. If Slack has a 30-second outage, we lose the alert â€” exactly when we need it most. And if 500 servers all breach a CPU threshold simultaneously, the on-call engineer gets 500 pages. Neither is acceptable."
Notification Service design:
The Alert Evaluator emits alert events to a Kafka topic first. The Notification Service consumes from that topic and handles four concerns:
1. Deduplication:
Tracks alert state as a state machine: inactive â†’ pending â†’ firing â†’ resolved

pending: condition is breached but for duration not yet elapsed
firing: condition has been breached continuously for the full duration â†’ notify
resolved: condition is no longer breached â†’ notify once (resolution message)
"This means one page when the problem starts, one when it ends. Not one page per evaluation cycle."
2. Grouping:
Collects alert events within a short time window (e.g., 30 seconds) and groups them by common labels â€” e.g., all cpu_usage alerts in region="us-east" become one notification: "47 hosts in us-east are above 90% CPU" instead of 47 separate pages.
3. Silencing:
Users can create silence rules â€” "suppress all alerts for host=server-42 between 2am-4am during maintenance". The Notification Service checks incoming alerts against active silences before dispatching.
4. Escalation:
If an alert fires and nobody acknowledges within N minutes, escalate to a secondary channel or person. Stored as a simple scheduled job â€” if no ack event arrives within the window, re-notify.
Reliability of delivery:
"Since alert events are in Kafka before any external call is made, delivery is durable. The Notification Service processes each event with at-least-once semantics. For each event, it attempts delivery to the external channel (Slack, PagerDuty, email). On failure, it retries with exponential backoff. The Kafka offset is only committed after successful delivery â€” so a crash mid-delivery replays the event on recovery."


ğŸ¤ Interviewer:
"What about the Notification Service itself going down?"
ğŸ‘¨â€ğŸ’» Candidate:
"Since all alert events are durably stored in Kafka, a Notification Service restart just resumes from its last committed offset â€” no events are lost. I'd run multiple instances in a consumer group for redundancy. The bigger risk is a prolonged outage where Kafka retention expires before we recover â€” so I'd set alert topic retention to at least 48 hours and have a runbook for that scenario. I'd also add meta-monitoring: a watchdog that independently checks whether the Notification Service is processing events, and pages through a separate out-of-band channel (e.g., a simple email via SES that doesn't go through our own stack) if it's falling behind."
ğŸ¤ Interviewer (classic curveball):
"How would you monitor the monitoring system?"
ğŸ‘¨â€ğŸ’» Candidate:
"Great question â€” and the wrong answer is to use the same monitoring system to monitor itself. If it's down, so is your monitoring of it. I'd use a separate, minimal 'meta-monitoring' system â€” ideally a completely independent stack, possibly a managed service like AWS CloudWatch or a simple external uptime checker. It watches for: ingestion lag in Kafka, alert evaluator heartbeats, notification service throughput, and TSDB write success rates. If any of these degrade, it notifies through an out-of-band channel â€” not through the system being monitored."
âœ… What makes this staff-level:

Starts simple (polling) and justifies when to add complexity (Flink only for real-time tier)
Treats notification delivery as a separate reliability problem from alert evaluation
Proactively covers deduplication, grouping, silencing, escalation â€” the real-world messiness
Addresses the "monitoring the monitor" meta-problem with a concrete answer


Ready for Deep Dive 3: High Availability & Handling Failures?
We'll cover:

Failure modes at each layer (agent, Kafka, TSDB, alert evaluator)
Graceful degradation vs. hard failures
Late/out-of-order data handling
Just say "go ahead"!


 Deep Dive 3: High Availability & Handling Failures

ğŸ¤ Interviewer:
"Monitoring systems are most critical exactly when everything else is on fire. How do you ensure this system stays up during failures and traffic spikes? And how do you handle late or out-of-order data?"
ğŸ‘¨â€ğŸ’» Candidate:
"Let me walk through the failure modes layer by layer â€” from the edge to the core â€” and then cover out-of-order data separately since it's a distinct problem."
Layer 1: Agent (Edge)
Failure mode: Network partition between server and ingestion layer. Agent can't flush metrics.
Solution:

Agents maintain a local disk buffer â€” if the flush fails, metrics are written to a local queue (bounded, e.g., 100MB)
Agent retries with exponential backoff + jitter
If the buffer fills up (extended outage), agents drop oldest data first â€” recent data is more valuable than old data for a monitoring system
When connectivity restores, agents flush the buffer and catch up
"This means short network blips are completely invisible to the system. Only sustained outages (hours) cause data loss, and even then it's graceful â€” we drop old data, not recent data."


Layer 2: Kafka
Failure mode: Broker failure, partition leader election, or a consumer falling behind.
Solution:

Replication factor of 3 across availability zones â€” a broker failure triggers automatic leader election with no data loss
ISR (In-Sync Replicas): Producers only get acknowledgment after all ISR replicas confirm the write â€” no data loss even if the leader dies immediately after ack
Consumer groups with multiple instances: If one consumer crashes, Kafka reassigns its partitions to another consumer in the group within seconds
Retention set to 48 hours: If all consumers are down, metrics accumulate in Kafka. When they recover, they replay and catch up.
The catch-up problem:
"If the system is down for 5 minutes, we have 5 minutes of backlog â€” about 1.5 billion metrics. At our normal processing rate we can consume this in 5 minutes additional time, so we'll be 10 minutes behind total. But this assumes we have spare capacity. If we're running at 80% utilization normally, catch-up takes much longer â€” a dangerous positive feedback loop. I'd design for 50% normal utilization specifically to have headroom for catch-up scenarios. And I'd have a circuit breaker: if lag exceeds a threshold, we start dropping lower-priority metrics to prioritize critical ones."


Layer 3: Ingestion Service
Failure mode: Ingestion service instances crash or are overwhelmed.
Solution:

Multiple instances behind a load balancer â€” stateless, so any instance can handle any request
Idempotent writes: Each metric data point includes a deterministic ID (hash of metric name + labels + timestamp). If a consumer crashes mid-batch and replays from Kafka, duplicate data points are detected and deduplicated at the storage layer
Health checks with automatic instance replacement via auto-scaling groups


Layer 4: Time-Series Database
Failure mode: Node failure, disk exhaustion, or slow queries impacting write throughput.
Solution:

Multi-node cluster with replication â€” writes go to a primary with synchronous replication to at least one replica. Reads can be served from replicas to reduce primary load.
Write path isolation from read path: Separate read replicas for dashboard queries so an expensive dashboard query can't starve the write path
Disk capacity: With our compression estimates (~5KB per series per day), we can calculate capacity precisely and set alerts when utilization exceeds 70%
Circuit breaker on writes: If the TSDB is overwhelmed, the ingestion service backs off gracefully rather than overwhelming it further â€” Kafka absorbs the backlog


Layer 5: Alert Evaluator & Notification Service
Failure mode: Alert Evaluator crashes mid-evaluation cycle. Notification Service fails to deliver.
Solution:

Alert Evaluator: Stateless â€” each evaluation cycle is independent. Multiple instances each own a shard of rules (by rule_id hash). If one crashes, its rules aren't evaluated until it recovers or another instance takes over. I'd add a heartbeat system: if a rule hasn't been evaluated in 2x its interval, a watchdog reassigns it.
Notification Service: As discussed in Deep Dive 2 â€” Kafka as a durable buffer, at-least-once delivery, retry with backoff. Alert events are never lost as long as Kafka is healthy.


Graceful Degradation Strategy
"A key staff-level insight: not all failures are equal. I'd design explicit degradation tiers:"
Component downImpactDegraded behaviorOne ingestion instanceNoneLoad balancer routes around itKafka partition leaderSeconds of delayAutomatic leader electionTSDB read replicaDashboard slownessFall back to primary, add cache TTLAlert Evaluator instanceSome rules delayedOther instances cover via watchdogNotification ServiceAlert delivery delayKafka buffers, retries on recoveryFull TSDB primaryNo writes or readsMetrics buffered in Kafka, dashboards serve stale cache"The system should never fully go dark. Even in a catastrophic TSDB failure, agents keep buffering locally, Kafka keeps accumulating, and dashboards serve cached data with a staleness warning. Alerts are delayed but not lost."

Out-of-Order & Late Data
ğŸ¤ Interviewer:
"How do you handle metrics that arrive late or out of order? This is common with network delays."
ğŸ‘¨â€ğŸ’» Candidate:
"This is subtle but important. Late data comes in two flavors:"
1. Slightly late (seconds to minutes):

Agent batches every 10 seconds, network adds a few seconds of jitter
The TSDB accepts writes for any timestamp â€” it just inserts them into the right time block
No special handling needed â€” the data point lands in the correct position in the series
2. Very late (hours or more):
A server was partitioned for 2 hours and now flushes its local buffer
This is problematic for rollups â€” if we've already computed the 1-hour rollup for that time block, the late data makes it stale
Solution: Rollup recomputation. The TSDB tracks the latest write timestamp per time block. If a write arrives for an already-rolled-up block, it marks that block as "dirty" and a background recomputation job re-rolls it.
For dashboards, queries can optionally specify include_late_data=true to re-scan raw data instead of using cached rollups â€” a trade-off between freshness and latency
3. Out-of-order within a series:
Modern LSM-based TSDBs handle this natively â€” they sort by timestamp within each time block during the compaction process
No special handling needed at the ingestion layer
ğŸ¤ Interviewer:
"Where do you draw the line? How late is too late to accept?"
ğŸ‘¨â€ğŸ’» Candidate:
"I'd configure a maximum out-of-order tolerance window â€” e.g., 2 hours. Data older than 2 hours is rejected at ingestion with a specific error code so the agent knows not to retry it. This prevents unbounded rollup recomputation and protects the system from a misbehaving agent that suddenly dumps days of buffered data. The 2-hour threshold is configurable per tenant â€” a high-value customer might get a longer window."


âœ… What makes this staff-level:

Thinks about every layer independently with specific failure modes and mitigations
Addresses the catch-up problem quantitatively â€” not just "Kafka buffers it" but "here's the math on why we need 50% headroom"
Has a graceful degradation strategy â€” the system never fully goes dark
Handles out-of-order data with nuance â€” distinguishes slightly late vs. very late vs. out-of-order and proposes appropriate solutions for each


Ready for Deep Dive 4: Cardinality Explosion?
This is one of the sneakiest and most interview-differentiating topics. We'll cover:

Why cardinality silently kills the system
Detection and enforcement mechanisms
The bloom filter optimization
Just say "go ahead"!

ğŸ“ˆ Deep Dive 4: Cardinality Explosion

ğŸ¤ Interviewer:
"You've mentioned cardinality a few times. Let's go deep on it. What exactly is the problem and how would you solve it?"
ğŸ‘¨â€ğŸ’» Candidate:
"Cardinality explosion is one of the sneakiest problems in metrics systems because it doesn't fail loudly â€” it degrades silently until the system falls over. Let me explain the problem from first principles before getting into solutions."
The Problem: What is Cardinality Explosion?
"Every unique combination of metric name + label set creates a new series. Each series has overhead in the time-series database:"

An index entry to look up the series by its labels
In-memory tracking of the current write buffer for that series
A per-series write buffer (WAL entry, in-memory chunk) that gets flushed to disk
"This overhead is small per series â€” maybe 1-5KB of memory. But it multiplies fast."
Example:
Take http_requests with these labels:
host â†’ 500,000 values
endpoint â†’ 200 values
status_code â†’ 10 values
method â†’ 5 values
Theoretical max series: 500,000 Ã— 200 Ã— 10 Ã— 5 = 5 billion series
Even at 1KB per series in memory: 5 petabytes of RAM â€” obviously impossible.
In practice it's far less (not every combination exists), but even 10 million active series Ã— 5KB = 50GB of RAM just for series metadata. At some point the TSDB starts swapping, index lookups slow down, write throughput collapses, and queries time out.
"The particularly nasty part: this degradation is gradual. The system works fine at 1M series, starts slowing at 5M, becomes unstable at 20M. By the time you notice, you're already in trouble."


Common Causes
"In practice, cardinality explosions are almost always caused by someone adding a high-cardinality label without thinking about the consequences:"

User IDs as labels: http_requests{user_id="abc123"} â€” instantly multiplies series by your entire user base
Request/trace IDs as labels: Similar problem â€” unbounded unique values
Timestamps as labels: Catastrophically bad â€” every data point creates a new series
Unbounded endpoint labels: /api/users/12345 vs /api/users/{id} â€” if not normalized, every user ID becomes a unique endpoint label


Detection
"You can't fix what you can't see. I'd build cardinality monitoring as a first-class feature:"
1. Per-metric series count tracking:

Maintain a counter in Redis for each metric name â†’ current series count
Expose this as a meta-metric: tsdb_series_count{metric="http_requests"} = 2,400,000
Alert when any metric exceeds a threshold (e.g., > 1M series)
2. Cardinality growth rate alerts:
If http_requests grows from 100K to 500K series in 10 minutes, something is wrong
A rate-of-change alert fires before the absolute limit is hit
3. Top-N cardinality report:
A daily job that ranks metrics by series count and emails the top 10 to the team
Forces visibility â€” teams often don't realize what they've shipped


Prevention & Enforcement
"Detection tells you when you have a problem. Prevention stops it before it starts."
1. Label allowlisting (schema enforcement):
Each metric has a registered schema in a Policy Store (Postgres):
metric: http_requests
allowed_labels: [host, region, endpoint, status_code, method]
max_series: 500,000
per_label_value_limits:
  endpoint: 1,000
  status_code: 50
At ingestion, the Ingestion Service:

Strips any label key not in the allowlist (unknown labels are dropped silently)
Rejects data points where a per-label value limit is exceeded
"This forces teams to explicitly register new labels through a process â€” a PR, a review, a capacity estimate. It creates a natural gate."
2. Series cap enforcement:
The Ingestion Service checks the current series count before accepting a new series:

For each incoming data point:
1. Hash(metric_name + sorted_label_set) â†’ series_id
2. Check Redis: SISMEMBER series_set:{metric_name} series_id
3. If new series:
   a. Check current count vs. cap
   b. If under cap: SADD series_set:{metric_name} series_id â†’ accept
   c. If over cap: drop data point, increment dropped_metrics counter, fire alert
4. If existing series: accept immediately
"The Redis SET membership check is O(1) and very fast â€” but at 5M metrics/sec, even a Redis round trip per data point is too slow."
ğŸ¤ Interviewer:
"5M Redis lookups per second is expensive. How do you optimize that?"
ğŸ‘¨â€ğŸ’» Candidate:
Bloom Filter Optimization:
"This is where it gets interesting. The hot path check â€” 'have I seen this series before?' â€” is a membership query. The vast majority of incoming data points are for existing series, not new ones. New series creation is rare. So we can use a probabilistic data structure to handle the common case cheaply."
A Bloom filter per metric sits in memory on each Ingestion Service instance:

If the Bloom filter says "definitely not seen" â†’ this is a new series â†’ go to Redis to confirm and add
If the Bloom filter says "probably seen" â†’ skip the Redis check, accept immediately
"Bloom filters have no false negatives â€” if a series exists, the filter will always say 'probably seen.' They can have false positives â€” occasionally saying 'probably seen' for a series that doesn't exist yet. But a false positive here just means we occasionally accept a new series without checking the cap, which is a minor over-count rather than a catastrophic failure."
Trade-offs:
Memory: A Bloom filter for 10M series with 1% false positive rate takes ~12MB â€” trivially cheap
Accuracy: Periodically sync the Bloom filter from Redis to correct drift (e.g., every 5 minutes)
Multi-instance: Each ingestion instance has its own local Bloom filter. New series might pass the filter on one instance but not another until sync â€” acceptable since cap enforcement is approximate anyway


Handling Cardinality Explosions in Production
"Even with all these controls, accidents happen. A bad deploy ships code with a user_id label. Series count jumps from 100K to 10M in 5 minutes. Here's the runbook:"

Automated cap kicks in: Series above the cap are dropped, dropped_metrics alert fires
Identify the offending label: Cardinality report shows http_requests exploded, label analysis points to user_id
Label stripping: Update the Policy Store to remove user_id from the allowlist â€” takes effect immediately at ingestion
TSDB cleanup: The millions of dead series don't disappear immediately â€” they stop receiving new data points but their metadata persists until the retention window expires. Most TSDBs have a "tombstone" mechanism to mark series as inactive and exclude them from query planning
Post-mortem: Add user_id-style patterns to a linter that runs in CI against metric registration PRs


ğŸ¤ Interviewer:
"What about multi-tenancy? If one team causes a cardinality explosion, should it affect other teams?"
ğŸ‘¨â€ğŸ’» Candidate:
"Absolutely not â€” this is a critical isolation requirement. I'd implement per-tenant cardinality budgets enforced at the ingestion layer. Each tenant gets a total series budget (e.g., 10M series) and the Policy Store tracks usage per tenant. A single team blowing their budget hits their own cap, not a global one. In the TSDB, tenant data is either physically isolated (separate namespaces or clusters for large tenants) or logically isolated with strict per-tenant query quotas so one tenant's expensive query can't starve another's. This is one of the hardest parts of building a SaaS monitoring platform â€” Datadog charges specifically based on custom metrics volume precisely because cardinality is so operationally expensive."
âœ… What makes this staff-level:

Explains cardinality from first principles â€” not just "it's bad" but why, with math
Covers detection, prevention, and recovery â€” the full lifecycle
Goes deep on the Bloom filter optimization â€” shows algorithmic thinking
Raises multi-tenancy isolation proactively â€” a real-world concern most candidates miss
Connects to business reality â€” Datadog's pricing model is a direct consequence of this problem


Ready for the final step â€” Step 6: Wrap-up & Trade-offs?
This is where you tie everything together, summarize your design, and show the interviewer you can zoom back out. Just say "go ahead"!

Step 6: Wrap-up & Trade-offs

ğŸ¤ Interviewer:
"We're coming up on time. Can you summarize your design, the key trade-offs you made, and what you'd do differently given more time?"
ğŸ‘¨â€ğŸ’» Candidate:
"Sure. Let me zoom back out and give you the full picture, then highlight the deliberate trade-offs."
ğŸ—ºï¸ Full System Summary
The system has three distinct paths, each designed for its own characteristics:
Write Path (optimized for throughput & durability):

Agents (local batching + buffering) â†’ Kafka (durable buffer, partitioned by series) â†’ Ingestion Service (validation, cardinality enforcement, idempotent writes) â†’ Time-Series DB (append-optimized, time-partitioned, compressed)
Read Path (optimized for low latency):
Dashboard/User â†’ Query Service (DSL translation, rollup tier selection) â†’ Redis Cache (query result cache, split historical/recent) â†’ Time-Series DB (rollup tables + raw data)
Alert Path (optimized for reliability):
Alert Evaluator (polling-based, sharded by rule) â†’ Kafka (durable alert event buffer) â†’ Notification Service (deduplication, grouping, silencing, escalation) â†’ Slack / PagerDuty / Email
Supporting components:


Postgres: Alert rules, cardinality policies, tenant configuration
Redis: Query cache, series cardinality tracker, Bloom filter sync
Rollup Worker: Background computation of 1-min / 1-hour / 1-day aggregates
Watchdog / Meta-monitor: Independent stack monitoring the health of the monitoring system itself


âš–ï¸ Key Trade-offs Made
1. Polling-based alerting over streaming (Flink)

Chose: Polling every 30-60 seconds
Trade-off: Up to 60 seconds alert latency vs. seconds with Flink
Rationale: Sub-minute is sufficient for the stated SLA. Polling is dramatically simpler to operate, debug, and reason about. Flink adds stateful stream processing complexity that's only justified for a premium real-time tier.
2. Eventual consistency for dashboards
Chose: Serve from Redis cache with rollups, tolerate seconds-to-minutes of staleness
Trade-off: Stale data on dashboards vs. always-fresh but slow queries
Rationale: Engineers debugging incidents care about trends, not millisecond-fresh data. Sub-second dashboard response time is worth more than perfect freshness.
3. Approximate cardinality enforcement (Bloom filter)
Chose: Bloom filter as first-pass check, Redis as ground truth
Trade-off: Occasional false positives (rare new series slip through) vs. exact enforcement with higher latency
Rationale: At 5M metrics/sec, an exact Redis lookup per data point is too expensive. The Bloom filter handles 99%+ of cases locally. Approximate cap enforcement is acceptable â€” being 0.1% over cap is not a crisis.
4. Agent-based collection over server push
Chose: Agents running on every server
Trade-off: Operational complexity (deploy, version, monitor 500K agents) vs. simplicity of direct push
Rationale: Local buffering, local aggregation, and 100x reduction in central ingestion load make this essential at our scale. Every production monitoring system uses this pattern.
5. Lossy rollups with histogram preservation
Chose: Pre-compute rollups at multiple resolutions, store histograms for percentile metrics
Trade-off: Increased storage complexity and background computation vs. always querying raw data
Rationale: Without rollups, long-range queries scan billions of data points and are unusably slow. Histograms preserve the distribution needed for accurate percentile computation from rollups.


ğŸš§ What I'd Do Differently Given More Time
1. Push vs. Pull collection model debate
"We designed a push model â€” agents push to our system. Prometheus uses a pull model â€” the monitoring system scrapes each server. Pull has interesting properties: the monitoring system controls the scrape rate, dead servers are automatically detected when scrapes fail, and there's no configuration on the server side. I'd want to discuss this trade-off more deeply â€” in practice, large-scale systems like Datadog use push because pull doesn't scale to 500K servers easily, but it's worth a conversation."
2. Multi-region replication
"We scoped this out, but a production monitoring system needs multi-region HA. Metrics data would be written to a primary region and async-replicated to a secondary. Alert evaluation would run in both regions independently, with deduplication at the Notification Service layer. This adds significant complexity â€” especially around split-brain scenarios â€” but is essential for a globally distributed fleet."
3. Schema registry for metrics
"Right now, teams register metric schemas manually in the Policy Store. I'd build a proper schema registry â€” similar to Confluent Schema Registry for Kafka â€” where metric schemas are versioned, validated, and evolved with backward compatibility rules. This prevents breaking changes and makes the cardinality enforcement more robust."
4. Cost optimization with tiered storage
"Older rollup data (> 90 days) could be moved to object storage (S3/GCS) in columnar format (Parquet). Queries over historical data would use a query engine like Athena or BigQuery. This dramatically reduces storage costs for cold data while keeping hot data in the TSDB for fast access."
5. Tenant-aware query routing
"In a multi-tenant SaaS deployment, large tenants with massive cardinality should get physically isolated TSDB clusters, not just logical isolation. Noisy neighbor protection at the storage layer is hard to retrofit â€” it needs to be designed in from the start."
ğŸ¯ Final Reflection (candidate zooms all the way out)
"The core insight of this design is that a metrics monitoring system is fundamentally three different problems that happen to share a storage layer:

An extreme write throughput problem â€” solved by edge batching, Kafka buffering, and write-optimized storage
A low-latency analytics problem â€” solved by rollups, columnar compression, and aggressive caching
A reliability and trust problem â€” solved by durable queuing, graceful degradation, and independent meta-monitoring
Most of the interesting design decisions come from keeping these three concerns separated rather than letting them collapse into a single tightly-coupled system. The moment you let dashboard query spikes affect ingestion, or let alert evaluation compete with write throughput, you've lost the properties that make the system trustworthy â€” and a monitoring system you can't trust is worse than no monitoring system at all."


ğŸ¤ Interviewer:
"Great. I have no more questions â€” strong design overall."
ğŸ End of Mock Interview

ğŸ“Š Overall Assessment
Here's how this performance maps to staff-level expectations:
AreaQualityNotesRequirements clarificationâœ… StrongConnected scale math to design decisionsCore entitiesâœ… StrongCardinality raised proactivelyHigh-level architectureâœ… StrongSeparated write/read/alert paths clearlyAPI designâœ… StrongJustified every design choiceStorage deep diveâœ… StrongRollups, histograms, LSM treesAlerting deep diveâœ… StrongPolling vs. Flink trade-off well arguedHA & failure handlingâœ… StrongCatch-up math, graceful degradation tiersCardinality deep diveâœ… StrongBloom filter, multi-tenancy, business connectionWrap-up & trade-offsâœ… StrongZoomed out to core insight

ğŸ’¬ How did you find the walkthrough? Any areas you'd like to revisit, go deeper on, or practice answering yourself?

This is really good.



Thank you, glad it was helpful! The design we walked through covers all the key dimensions of a staff-level system design interview â€” not just what to build, but why each decision was made, what the trade-offs are, and how the pieces connect to the underlying scale constraints. A few tips if you're preparing for a real interview with this problem: Practice out loud. Reading through a design is very different from articulating it under pressure. Try explaining each section to yourself or a friend without notes â€” you'll quickly find the parts you haven't fully internalized. Own your opinions. At the staff level, interviewers want to hear "I'd choose X over Y because..." not just "one option is X, another is Y." The Bloom filter optimization, the polling-vs-Flink argument, the catch-up capacity math â€” these are the moments that differentiate a staff answer. Don't skip the numbers. The back-of-the-envelope math in Step 1 isn't just a warmup â€” it anchors every design decision that follows. Interviewers notice when candidates make design choices that aren't grounded in the actual scale of the problem. The final insight matters. Ending with "here's the core principle that ties everything together" shows systems thinking, not just systems knowledge. That's what staff-level is really about. Good luck with your interview! Feel free to come back if you want to practice any other system design problems or do a version where you answer and I give feedback.




