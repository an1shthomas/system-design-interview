# Design a Metrics Monitoring Platform (Datadog)

---

## Step 1: Clarifying Requirements

**ðŸŽ¤ Interviewer:** "Design a metrics monitoring platform like Datadog. Where would you like to start?"

**ðŸ‘¨â€ðŸ’» Candidate:** "Before jumping into the design, I'd like to ask a few clarifying questions."

> **Functional scope:**
> - "Are we covering the full observability stack â€” metrics, logs, and traces â€” or just metrics?" â†’ *Just metrics*
> - "Should we support both infrastructure metrics (CPU/memory) and custom application metrics (request counts, business counters)?" â†’ *Yes, both*
> - "For dashboards, are we building the visualization layer or just the backend query API?" â†’ *Just the backend â€” API + storage*
> - "For alerting, what notification channels? Slack, PagerDuty, email?" â†’ *Yes, all three*
>
> **Scale:**
> - "How large is the fleet we're monitoring?" â†’ *500,000 servers*
> - "How frequently do servers emit metrics?" â†’ *Every 10 seconds*
> - "How long do we need to retain data?" â†’ *1 year*
>
> **Consistency & latency:**
> - "How quickly do alerts need to fire after a breach?" â†’ *Sub-minute is fine*
> - "For dashboards, are stale reads okay?" â†’ *Eventual consistency is fine*

**ðŸ“ Back-of-the-envelope (candidate does this out loud):**

"Let me quickly size the problem so our design decisions are grounded in numbers."

| Parameter | Value |
|---|---|
| Servers | 500,000 |
| Metrics per server | 100 |
| Emit frequency | Every 10s |
| **Metrics/second** | **500K Ã— 100 / 10 = 5M/s** |
| Bytes per data point | ~150B (name ~20B + timestamp 8B + value 8B + labels ~100B) |
| Raw ingestion rate | ~750 MB/s |
| Raw data per day | ~65 TB â†’ ~3â€“6 TB with 10â€“20x compression |
| 1 year (with rollups) | ~1â€“2 PB, manageable with tiered retention |

> "This is clearly a write-heavy system with bursty reads â€” engineers query during incidents. These two patterns need to be designed independently."

**ðŸŽ¤ Interviewer:** "What are the most important non-functional requirements given that scale?"

**ðŸ‘¨â€ðŸ’» Candidate:**
- High write throughput â€” 5M metrics/sec, must not drop data
- Low-latency reads â€” dashboard queries should return in seconds even over weeks of data
- Alert reliability â€” alerts must fire even if parts of the system are degraded
- High availability â€” the monitoring system is most critical exactly when things are going wrong
- Cardinality control â€” unbounded label combinations can silently kill the system

> **âœ… Key insight a staff engineer shows here:**
> Don't just list requirements mechanically â€” connect the scale math to design consequences ("write-heavy and read-bursty means we separate these paths") and proactively name cardinality as a non-obvious but critical constraint.

---

## Step 2: Core Entities & Data Modeling

**ðŸŽ¤ Interviewer:** "Before we get into the architecture, walk me through the core entities and how they relate."

**ðŸ‘¨â€ðŸ’» Candidate:** "Let me identify the key 'nouns' in the system â€” getting this right avoids a lot of confusion later."

```
Label       â†’ key-value pair giving context  e.g. host="server-1", region="us-east"
Metric      â†’ name + labels + value + timestamp
              e.g. cpu_usage{host="server-1", region="us-east"} = 0.73 at t=1640000000
Series      â†’ unique (metric name + label set) â†’ sequence of data points over time
              cpu_usage{host="server-1"} over 30 days = one series
              cpu_usage{host="server-2"}             = a different series
Data Point  â†’ single (timestamp, value) entry â€” written at 5M/sec
Alert Rule  â†’ query + threshold + for-duration + notification channels
Dashboard   â†’ collection of panels backed by metric queries (drives the read path)
```

> "A 'series' is the atomic unit of storage. The number of unique series â€” called **cardinality** â€” is the primary scaling challenge. I'll come back to this in the deep dive."

**ðŸŽ¤ Interviewer:** "You mentioned cardinality. Can you give a quick example of how it can explode?"

**ðŸ‘¨â€ðŸ’» Candidate:** "Take `http_requests` with labels: `host` (500K values) Ã— `endpoint` (200) Ã— `status_code` (10) Ã— `method` (5):

500,000 Ã— 200 Ã— 10 Ã— 5 = **5 billion** theoretical series

Each series has overhead in the storage engine â€” its own index entry, in-memory tracking, and write buffer. At that scale the database degrades silently: write throughput drops, memory spikes, queries slow down. This is why cardinality is a first-class design concern, not an afterthought."

> **âœ… What makes this staff-level:**
> - Connects entities to the scaling problem (series â†’ cardinality)
> - Doesn't just list entities â€” explains why each one matters to the design
> - Proactively foreshadows the cardinality deep dive without getting lost in it yet

---

## Step 3: High-Level Architecture

**ðŸŽ¤ Interviewer:** "Walk me through the high-level architecture. How does data flow end to end?"

**ðŸ‘¨â€ðŸ’» Candidate:** "I'll split the system into three paths â€” write, read, and alert â€” because they have fundamentally different characteristics and need to be designed independently."

```
[Servers]
    â†“  agent (local batching every 10s)
[Kafka]  â†â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” [Cardinality Enforcer (Redis + Bloom Filter)]
    â†“  ingestion consumers
[Ingestion Service]
    â†“
[Time-Series DB] â†â†’ [Rollup Worker]
    â†‘
[Query Service] â†â†’ [Redis Cache]
    â†‘
[Dashboard / Users]

[Alert Rules DB (Postgres)]
    â†“
[Alert Evaluator] â€”â€”pollsâ€”â€”â†’ [Time-Series DB]
    â†“
[Notification Service] â†’ Kafka â†’ Slack / PagerDuty / Email
```

### âœï¸ Write Path

> "The simplest approach: each server POSTs metrics directly to an ingestion service which writes to a database. This fails at 5M metrics/sec â€” the ingestion service is a bottleneck and the database gets hammered with no buffer."

**Real approach:** `Agent â†’ Kafka â†’ Ingestion Consumers â†’ Time-Series DB`

- **Agent** (on every server): lightweight daemon that collects metrics locally, batches every 10s, flushes to Kafka. Instead of 5M individual writes/sec hitting a central service, agents batch locally â†’ **~50K batched requests/sec**. 100x load reduction at the edge.
- **Kafka**: durable high-throughput buffer. Partitioned by `hash(metric_name + labels)` so the same series always lands on the same partition (important for ordering). Gives us backpressure handling, durability, and replay capability.
- **Ingestion Service**: validates data points, enforces cardinality limits, writes batches to the TSDB.
- **Time-Series DB**: append-optimized, time-partitioned, columnar compression.

### ðŸ“– Read Path

`User â†’ Query Service â†’ Redis Cache â†’ Time-Series DB`

- **Query Service**: accepts a PromQL-like DSL, selects the appropriate rollup resolution based on the requested `step`, returns formatted results.
- **Redis Cache**: caches query results keyed by `hash(query + time_range)`. Hit rates are very high â€” a 24-hour dashboard window queried every 30s shifts by only 30s.

### ðŸ”” Alert Path

`Alert Evaluator â†’ (polls Time-Series DB) â†’ Notification Service â†’ Slack / PagerDuty / Email`

- **Alert Evaluator**: fetches active alert rules from Postgres every ~30s, executes metric queries, checks if thresholds are breached for the required duration.
- **Notification Service**: handles deduplication, grouping, silencing, and escalation. Writes alert events to Kafka before dispatching to external channels â€” if Slack is down, the alert isn't lost.

**ðŸŽ¤ Interviewer:** "Why Kafka? Couldn't you use a load-balanced ingestion service with auto-scaling?"

**ðŸ‘¨â€ðŸ’» Candidate:** "Auto-scaling helps the ingestion service but just moves the bottleneck downstream to the database â€” 50 instances all hammering storage simultaneously with no buffer. Kafka decouples them. It also gives durability: if the DB slows for 2 minutes, Kafka holds those metrics and consumers catch up. Without it, you're dropping data during an incident â€” exactly when you need it most."

**ðŸŽ¤ Interviewer:** "What's the trade-off of agents vs. servers pushing directly?"

**ðŸ‘¨â€ðŸ’» Candidate:** "The main trade-off is operational complexity â€” you now have a daemon to deploy, version, configure, and monitor on 500K servers. But the benefits outweigh it: local buffering (metrics survive brief network outages), local aggregation (compute percentiles at the edge), and 100x reduction in central ingestion load. Every production-grade monitoring system â€” Datadog, Prometheus, OTEL â€” uses this pattern for exactly these reasons."

> **âœ… What makes this staff-level:**
> - Starts with the naive approach and explains why it fails before proposing the real solution
> - Clearly separates write, read, and alert paths with different design rationale for each
> - Justifies every component with trade-offs, not just "I'd use Kafka because it's good"
> - Connects decisions back to the scale numbers established in Step 1

---

## Step 4: API Design

**ðŸŽ¤ Interviewer:** "Let's define the API. What interfaces does this system expose and to whom?"

**ðŸ‘¨â€ðŸ’» Candidate:** "There are three distinct APIs serving different clients with very different usage patterns."

### ðŸ“¥ Ingestion API
*Client: Agents (machine-to-machine, extremely high volume)*

```
POST /v1/metrics/ingest
Content-Type: application/x-protobuf

{
  "metrics": [
    { "name": "cpu_usage",
      "labels": { "host": "server-1", "region": "us-east" },
      "value": 0.73,
      "timestamp": 1640000000 }
  ]
}

â†’ 202 Accepted
```

| Decision | Reason |
|---|---|
| Protobuf, not JSON | 3â€“10x smaller on the wire, faster to parse. At 50K req/s this matters. |
| Batched requests | Amortizes HTTP overhead across many metrics. |
| `202 Accepted` | Async â€” publishes to Kafka and returns immediately, does not wait for DB write. |
| Timestamps from the agent | Preserves actual observation time, handles late/out-of-order data correctly. |

### ðŸ“Š Query API
*Client: Dashboards, on-call engineers (human-driven, bursty, read-heavy)*

```
GET /v1/metrics/query
  ?query=avg(cpu_usage{region="us-east"})
  &start=1640000000
  &end=1640086400
  &step=60

â†’ 200 OK
{
  "metric": "cpu_usage",
  "labels": { "region": "us-east" },
  "datapoints": [
    { "timestamp": 1640000000, "value": 0.71 },
    ...
  ]
}
```

| Decision | Reason |
|---|---|
| GET, not POST | Idempotent and cacheable â€” Redis uses the full URL as cache key. |
| `step` parameter | Query service uses this to select the right rollup tier (e.g., 30-day + `step=3600` â†’ hourly rollup, not raw points). |
| PromQL-like DSL | Allows the query service to optimize execution â€” push filters to storage, pick the right rollup resolution. |

### ðŸ”” Alert Rules API
*Client: Engineers configuring monitoring (low volume, write-once-read-many)*

```
POST /v1/alerts/rules
{
  "name": "High CPU - US East",
  "query": "avg(cpu_usage{region='us-east'}) > 0.9",
  "for": "5m",
  "severity": "critical",
  "notifications": ["slack:#oncall-infra", "pagerduty:team-platform"]
}
â†’ 201 Created  { "rule_id": "rule_abc123" }

GET    /v1/alerts/rules               â†’ list all rules
GET    /v1/alerts/rules/{rule_id}     â†’ get one rule
PUT    /v1/alerts/rules/{rule_id}     â†’ update a rule
DELETE /v1/alerts/rules/{rule_id}     â†’ delete a rule
GET    /v1/alerts/active              â†’ currently firing alerts
```

| Decision | Reason |
|---|---|
| `for` duration | Prevents flapping â€” alert only fires if condition is continuously breached for the full duration. |
| `severity` | Notification Service routes differently â€” critical â†’ PagerDuty immediately, warning â†’ Slack. |
| Rule definition â‰  alert state | `POST /alerts/rules` defines the condition. `GET /alerts/active` shows what's currently firing. Stored separately. |

**ðŸŽ¤ Interviewer:** "You mentioned protobuf for ingestion. What about the query API?"

**ðŸ‘¨â€ðŸ’» Candidate:** "For the query API, JSON is fine. The volume is much lower â€” human-driven, maybe thousands of requests per minute at peak. Readability and debuggability outweigh wire efficiency. I'd only reach for protobuf on the hot path where volume justifies the added complexity."

**ðŸŽ¤ Interviewer:** "How would you handle auth and rate limiting on the ingestion API?"

**ðŸ‘¨â€ðŸ’» Candidate:** "Each agent authenticates with an API key scoped to a tenant, passed as a header. An API gateway in front handles auth validation and rate limiting per key â€” two purposes: protect the system from runaway agents, and enforce fair usage in a multi-tenant setup. I'd put this in the gateway, not the ingestion service itself, so the ingestion service stays focused purely on throughput."

> **âœ… What makes this staff-level:**
> - Justifies every design decision â€” protobuf vs. JSON, 202 vs. 200, GET vs. POST
> - Thinks about who the client is and designs the API accordingly (agent vs. human)
> - Proactively raises auth, rate limiting, and caching as first-class concerns
> - Connects API design back to the system's scaling constraints

---

## Step 5: Deep Dives

### ðŸ—„ï¸ Deep Dive 1: Storage & Query Performance

**ðŸŽ¤ Interviewer:** "You mentioned a time-series database. Why not just use Postgres?"

**ðŸ‘¨â€ðŸ’» Candidate:**

> "Let me walk through why Postgres breaks at our scale before explaining what we'd use instead."

- At 5M writes/sec, Postgres B-tree indexes become a bottleneck â€” every insert updates the index; B-trees don't handle append-heavy workloads well
- Queries like "avg CPU over 30 days for 1,000 servers" require scanning billions of rows â€” painfully slow even with indexes
- Retention management (deleting old data) causes table bloat, autovacuum pressure, and write amplification
- Row-by-row storage wastes space â€” time-series data compresses dramatically when stored together

**Why a Time-Series DB (InfluxDB, VictoriaMetrics, TimescaleDB):**

| Property | Mechanism |
|---|---|
| Append-only writes | LSM-tree storage â€” sequential appends, no random I/O, no index update overhead |
| Time-based partitioning | Data chunked into time blocks; drop old data = delete a block (no vacuum, no fragmentation) |
| Columnar compression | Delta encoding for timestamps, XOR (Gorilla) for values â†’ ~20x compression |

**Rollup strategy:**

| Resolution | Retention | Use Case |
|---|---|---|
| Raw (10s) | 2 days | Debugging recent incidents |
| 1-minute rollup | 2 weeks | Short-term trend analysis |
| 1-hour rollup | 90 days | Capacity planning |
| 1-day rollup | 2 years | Long-term trends / compliance |

A background **Rollup Worker** continuously computes aggregates (min, max, avg, count, sum + histograms for percentiles) from raw data.

**ðŸŽ¤ Interviewer:** "What about percentile metrics like p99 latency â€” can you compute those from rollups?"

**ðŸ‘¨â€ðŸ’» Candidate:** "This is a subtle but important point. You **cannot** accurately compute percentiles from pre-aggregated averages â€” taking the average of averages loses the distribution. The right approach is to store **histograms** at each rollup level. Each data point for a latency metric is bucketed (0â€“10ms, 10â€“50ms, 50â€“100ms, etc.), and the histogram is what gets rolled up. From a histogram you can estimate any percentile at query time. This is exactly how Prometheus histograms and Datadog distributions work."

**ðŸŽ¤ Interviewer:** "How do you handle low-latency dashboard queries over weeks of data?"

**ðŸ‘¨â€ðŸ’» Candidate:** "Three layers working together:
1. **Rollups** â€” the right resolution means fewer data points to scan
2. **Redis caching** â€” split queries into a cached historical chunk and a fresh recent chunk. The historical part is a cache hit; only the last few minutes hit the DB. Most dashboard queries are served from Redis in under 50ms.
3. **Pre-computation** â€” a background job identifies frequently-executed queries and pre-computes results on a schedule, warming the cache before users ask."

**Sharding:** Hash by `metric_name + label_set`. All data points for one series land on the same shard â€” no cross-shard fan-out for single-series time-range queries. Cross-series aggregations fan out in parallel and merge at the query layer.

---

### ðŸ”” Deep Dive 2: Alerting & Notifications

**ðŸŽ¤ Interviewer:** "Walk me through the alerting system in detail."

**ðŸ‘¨â€ðŸ’» Candidate:** "I'll split this into two parts: alert evaluation and notification delivery â€” they're separate concerns with different failure modes."

#### Part A: Alert Evaluation

**Polling approach (baseline):**
- Fetches all active alert rules from Postgres (cached in-memory, refreshed every ~30s)
- Executes the metric query against the TSDB for each rule
- Checks if the condition is breached continuously for the `for` duration
- At 10,000 rules every 30s â†’ ~333 queries/sec â€” very manageable
- Run multiple instances, each owning a shard of rules (by `rule_id` hash) for redundancy

**ðŸŽ¤ Interviewer:** "What if someone needs alerts in under 5 seconds?"

**ðŸ‘¨â€ðŸ’» Candidate:** "Polling fundamentally can't go below its interval. For sub-10-second alerting we need stream processing with **Flink**:
- Flink runs as a second consumer on the same Kafka topic
- Maintains windowed in-memory state per series (e.g., 5-minute sliding window)
- Evaluates conditions against that state â€” no DB query needed
- Latency: 2â€“5 seconds after the metric arrives in Kafka, vs. up to 60s with polling

Trade-offs to call out proactively:
- **Operational complexity**: Flink clusters need checkpointing, state management, careful failure recovery
- **Rule changes are hard**: updating a rule mid-stream without losing in-flight window state is non-trivial
- **Memory pressure**: windowed state for millions of series across thousands of rules needs careful capacity planning

My recommendation: use polling for the vast majority of alerts â€” simpler, cheaper, sub-minute is fine. Reserve Flink for a premium 'real-time alerts' tier users explicitly opt into. This is actually how Datadog structures it."

#### Part B: Notification Delivery

**ðŸŽ¤ Interviewer:** "How do you make notifications reliable without overwhelming on-call engineers?"

**ðŸ‘¨â€ðŸ’» Candidate:** "Having the Alert Evaluator call Slack directly is fragile â€” if Slack has a 30-second outage, we lose the alert exactly when we need it. And if 500 servers breach a CPU threshold simultaneously, the engineer gets 500 pages. Neither is acceptable."

The Alert Evaluator emits events to Kafka. The **Notification Service** consumes from that topic and handles:

| Concern | Mechanism |
|---|---|
| **Deduplication** | Alert state machine: `inactive â†’ pending â†’ firing â†’ resolved`. One page when it starts, one when it ends â€” not one per evaluation cycle. |
| **Grouping** | Collect events in a 30s window, group by common labels â€” e.g., 47 CPU alerts in `us-east` â†’ one notification. |
| **Silencing** | Check incoming alerts against active silence rules before dispatching. |
| **Escalation** | If no ack within N minutes, re-notify via escalation channel. |

Reliability: since alert events are durably in Kafka, the Notification Service uses at-least-once delivery â€” Kafka offset only committed after successful delivery. A crash mid-delivery replays on recovery.

**ðŸŽ¤ Interviewer:** "What about the Notification Service itself going down?"

**ðŸ‘¨â€ðŸ’» Candidate:** "It just resumes from its last committed offset â€” no events are lost. I'd run multiple instances in a consumer group. The bigger risk is a prolonged outage where Kafka retention expires â€” so I'd set alert topic retention to 48 hours. I'd also add **meta-monitoring**: a watchdog that independently checks whether the Notification Service is processing events, and pages through a completely separate out-of-band channel (e.g., direct SES email that doesn't go through our own stack) if it falls behind."

**ðŸŽ¤ Interviewer:** "How would you monitor the monitoring system?"

**ðŸ‘¨â€ðŸ’» Candidate:** "The wrong answer is to use the same monitoring system to monitor itself â€” if it's down, so is your monitoring of it. I'd use a completely separate, minimal meta-monitoring stack â€” possibly a managed service like AWS CloudWatch or a simple external uptime checker. It watches for: ingestion lag in Kafka, alert evaluator heartbeats, notification service throughput, and TSDB write success rates. Notifies through an out-of-band channel, not through the system being monitored."

> **âœ… What makes this staff-level:**
> - Starts simple (polling) and justifies when to add complexity (Flink only for real-time tier)
> - Treats notification delivery as a separate reliability problem from alert evaluation
> - Proactively covers deduplication, grouping, silencing, escalation â€” the real-world messiness
> - Addresses the "monitoring the monitor" meta-problem with a concrete answer

---

### âš¡ Deep Dive 3: High Availability & Failure Handling

**ðŸŽ¤ Interviewer:** "Monitoring systems are most critical exactly when everything else is on fire. How do you ensure this stays up during failures? And how do you handle late or out-of-order data?"

**ðŸ‘¨â€ðŸ’» Candidate:** "Let me walk through failure modes layer by layer, then cover out-of-order data separately."

| Layer | Failure Mode | Mitigation |
|---|---|---|
| **Agent** | Network partition | Local disk buffer (bounded, e.g., 100MB). Drop oldest first on overflow â€” recent data is more valuable. Retry with exponential backoff + jitter. |
| **Kafka** | Broker failure | Replication factor 3 across AZs. ISR acks before producer confirmation. Consumer group reassigns partitions on crash within seconds. 48h retention for catch-up. |
| **Ingestion Service** | Instance crash | Stateless â€” LB routes around it. Idempotent writes: deterministic `hash(name + labels + timestamp)` deduplicates replayed batches. |
| **Time-Series DB** | Node failure / slow queries | Multi-node with replication. Separate read replicas for dashboards so expensive queries can't starve the write path. Circuit breaker: back off gracefully rather than overwhelming the DB. |
| **Alert Evaluator** | Crash mid-cycle | Stateless per cycle. Watchdog reassigns rules if any rule goes unevaluated for 2Ã— its interval. |
| **Notification Service** | Crash mid-delivery | Resumes from last committed Kafka offset on restart. Run multiple instances in a consumer group. |

**The catch-up problem:**

> "5 minutes of downtime = ~1.5B metrics backlog. At 100% normal utilization, catch-up takes another 5 minutes (10 min total behind). At 80%, it takes much longer â€” a dangerous positive feedback loop. **Design for 50% normal utilization** specifically to have headroom for catch-up. Add a circuit breaker: if Kafka lag exceeds a threshold, start dropping lower-priority metrics to protect critical ones."

**Graceful degradation tiers:**

| Component Down | Impact | Degraded Behavior |
|---|---|---|
| One ingestion instance | None | LB routes around it |
| Kafka partition leader | Seconds of delay | Automatic leader election |
| TSDB read replica | Dashboard slowness | Fall back to primary, extend cache TTL |
| Alert Evaluator instance | Some rules delayed | Other instances cover via watchdog |
| Notification Service | Alert delivery delayed | Kafka buffers, retries on recovery |
| Full TSDB primary | No writes or reads | Metrics buffer in Kafka; dashboards serve stale cache with staleness warning |

> "The system should never fully go dark. Even in a catastrophic TSDB failure, agents keep buffering locally, Kafka keeps accumulating, and dashboards serve cached data with a staleness warning."

**ðŸŽ¤ Interviewer:** "How do you handle metrics that arrive late or out of order?"

**ðŸ‘¨â€ðŸ’» Candidate:** "Late data comes in two flavors:"

| Lateness | Handling |
|---|---|
| Slightly late (secondsâ€“minutes) | TSDB accepts writes for any timestamp â€” inserts into the right time block. No special handling needed. |
| Very late (hours â€” e.g., agent buffer flush after partition) | Mark affected rollup blocks as "dirty". Background recomputation job re-rolls them. Queries can optionally specify `include_late_data=true` to re-scan raw data instead of rollups. |
| Out-of-order within a series | LSM-based TSDBs handle this natively â€” sort by timestamp during compaction. No special handling at ingestion. |

**ðŸŽ¤ Interviewer:** "Where do you draw the line? How late is too late?"

**ðŸ‘¨â€ðŸ’» Candidate:** "I'd configure a maximum out-of-order tolerance window â€” e.g., 2 hours. Data older than that is rejected at ingestion with a specific error code so the agent knows not to retry. This prevents unbounded rollup recomputation and protects against a misbehaving agent suddenly dumping days of buffered data. The 2-hour threshold is configurable per tenant â€” a high-value customer might get a longer window."

> **âœ… What makes this staff-level:**
> - Thinks about every layer independently with specific failure modes and mitigations
> - Addresses the catch-up problem quantitatively â€” not just "Kafka buffers it" but "here's the math on why we need 50% headroom"
> - Has a graceful degradation strategy â€” the system never fully goes dark
> - Handles out-of-order data with nuance â€” distinguishes slightly late vs. very late vs. out-of-order and proposes appropriate solutions for each

---

### ðŸ“ˆ Deep Dive 4: Cardinality Explosion

**ðŸŽ¤ Interviewer:** "You've mentioned cardinality a few times. Let's go deep. What exactly is the problem and how would you solve it?"

**ðŸ‘¨â€ðŸ’» Candidate:** "Cardinality explosion is one of the sneakiest problems in metrics systems because it doesn't fail loudly â€” it degrades silently until the system falls over."

**The problem:**

Every unique `(metric_name + label_set)` creates a new series. Each series has TSDB overhead: an index entry, in-memory write buffer, and WAL entry â€” maybe 1â€“5KB per series. At 10M active series Ã— 5KB = 50GB of RAM just for series metadata. At some point the TSDB starts swapping, index lookups slow down, write throughput collapses, queries time out.

> "The particularly nasty part: degradation is gradual. Works fine at 1M series, starts slowing at 5M, becomes unstable at 20M. By the time you notice, you're already in trouble."

**Common causes:**
- User IDs / request IDs / trace IDs as label values â€” unbounded unique values
- Unnormalized URL paths as `endpoint` labels (`/api/users/12345` vs. `/api/users/{id}`)
- Timestamps as label values â€” catastrophic

**Detection:**

- **Per-metric series count** tracked in Redis, exposed as a meta-metric `tsdb_series_count{metric="http_requests"}` â€” alert if > threshold
- **Rate-of-change alerts**: fire before the absolute limit is hit (e.g., if `http_requests` grows from 100K to 500K series in 10 minutes)
- **Daily top-N cardinality report**: ranks metrics by series count, emails the top 10 â€” forces visibility

**Prevention â€” Policy Store (Postgres):**

```yaml
metric: http_requests
allowed_labels: [host, region, endpoint, status_code, method]
max_series: 500,000
per_label_value_limits:
  endpoint: 1000
  status_code: 50
```

Ingestion Service strips unknown label keys silently and rejects data points exceeding per-label value limits. Teams must explicitly register new labels through a review process â€” a natural gate.

**Series cap enforcement:**

```
For each incoming data point:
  series_id = hash(metric_name + sorted_label_set)
  if SISMEMBER series_set:{metric_name} series_id  â†’  accept (existing series)
  else:
    if current_count < cap  â†’  SADD + accept
    else                    â†’  drop + increment dropped_metrics + fire alert
```

**ðŸŽ¤ Interviewer:** "5M Redis lookups per second is expensive. How do you optimize that?"

**ðŸ‘¨â€ðŸ’» Candidate:** "This is where it gets interesting. The hot-path check â€” 'have I seen this series before?' â€” is a membership query. The vast majority of incoming data points are for **existing** series. New series creation is rare. So we use a **Bloom filter** per metric in memory on each Ingestion Service instance:

- `"definitely not seen"` â†’ new series â†’ go to Redis to confirm and register
- `"probably seen"` â†’ skip Redis, accept immediately

Bloom filters have no false negatives â€” existing series always pass. False positives occasionally let a new series slip through the cap check â€” minor over-count, not catastrophic.

- Memory: ~12MB for 10M series at 1% false positive rate
- Sync from Redis every ~5 minutes to correct drift
- Each ingestion instance has its own local Bloom filter â€” acceptable since cap enforcement is approximate anyway"

**Recovery runbook when cardinality explodes in production:**
1. Automated cap enforcement kicks in â†’ `dropped_metrics` alert fires
2. Cardinality report identifies the offending metric and label (e.g., `user_id`)
3. Update Policy Store allowlist to remove the bad label â†’ takes effect at ingestion immediately
4. TSDB cleanup: dead series stop receiving data; tombstone them to exclude from query planning before the retention window expires
5. Post-mortem: add label pattern linter to CI for metric registration PRs

**ðŸŽ¤ Interviewer:** "What about multi-tenancy â€” if one team causes a cardinality explosion, should it affect others?"

**ðŸ‘¨â€ðŸ’» Candidate:** "Absolutely not. Per-tenant cardinality budgets enforced at ingestion â€” one team hitting their budget hits their own cap, not a global one. In the TSDB, large tenants get physically isolated clusters. Strict per-tenant query quotas prevent one tenant's expensive query from starving another's. This is one of the hardest parts of building a SaaS monitoring platform â€” Datadog charges by custom metrics volume precisely because cardinality is so operationally expensive."

> **âœ… What makes this staff-level:**
> - Explains cardinality from first principles â€” not just "it's bad" but why, with math
> - Covers detection, prevention, and recovery â€” the full lifecycle
> - Goes deep on the Bloom filter optimization â€” shows algorithmic thinking
> - Raises multi-tenancy isolation proactively â€” a real-world concern most candidates miss
> - Connects to business reality â€” Datadog's pricing model is a direct consequence of this problem

---

## Step 6: Wrap-up & Trade-offs

**ðŸŽ¤ Interviewer:** "We're coming up on time. Summarize your design, the key trade-offs, and what you'd do differently given more time."

**ðŸ‘¨â€ðŸ’» Candidate:**

### System Summary

**Write path** (optimized for throughput & durability):
`Agents â†’ Kafka â†’ Ingestion Service (cardinality enforcement, idempotent writes) â†’ Time-Series DB`

**Read path** (optimized for low latency):
`User â†’ Query Service (DSL translation, rollup tier selection) â†’ Redis Cache â†’ Time-Series DB (rollups + raw)`

**Alert path** (optimized for reliability):
`Alert Evaluator (sharded, polling) â†’ Kafka â†’ Notification Service (dedup, grouping, silencing, escalation) â†’ Slack / PagerDuty / Email`

**Supporting components:**
- **Postgres**: alert rules, cardinality policies, tenant config
- **Redis**: query cache, series cardinality tracker, Bloom filter sync
- **Rollup Worker**: background computation of 1-min / 1-hour / 1-day aggregates
- **Meta-monitor**: independent stack watching ingestion lag, evaluator heartbeats, notification throughput

### Key Trade-offs

| Decision | Choice | Rationale |
|---|---|---|
| Alert evaluation | Polling (30â€“60s) | Sub-minute SLA is met. Flink reserved for a premium real-time tier â€” it's dramatically simpler to operate. |
| Dashboard freshness | Eventual consistency (cache + rollups) | Sub-second response time beats perfect freshness during incidents. |
| Cardinality enforcement | Bloom filter (approximate) | Exact Redis lookup per data point too expensive at 5M/s. 0.1% over cap is not a crisis. |
| Collection model | Agent-based push | 100x ingestion load reduction, local buffering, every production monitoring system uses this. |
| Long-range queries | Rollups with histograms | Without rollups, long-range queries scan billions of raw points. Histograms preserve percentile accuracy. |

### What I'd do differently given more time

- **Push vs. pull debate**: Prometheus uses pull (monitoring system scrapes each server) â€” interesting properties (dead servers auto-detected, no server-side config) but doesn't scale easily to 500K servers
- **Multi-region replication**: metrics written to primary region, async-replicated to secondary; alert evaluation in both with deduplication at the Notification Service layer
- **Schema registry**: versioned metric schemas with backward compatibility rules, similar to Confluent Schema Registry â€” prevents breaking changes
- **Tiered storage**: move rollup data > 90 days to object storage (S3/GCS) as Parquet, query with Athena or BigQuery â€” dramatically reduces storage costs for cold data
- **Tenant-aware query routing**: large tenants with massive cardinality should get physically isolated TSDB clusters â€” hard to retrofit, needs to be designed in from the start

---

## Core Insight

> A metrics monitoring system is fundamentally **three different problems** that happen to share a storage layer:
> 1. **Extreme write throughput** â†’ solved by edge batching, Kafka buffering, write-optimized storage
> 2. **Low-latency analytics** â†’ solved by rollups, columnar compression, aggressive caching
> 3. **Reliability and trust** â†’ solved by durable queuing, graceful degradation, independent meta-monitoring
>
> The moment you let dashboard query spikes affect ingestion, or let alert evaluation compete with write throughput, you've lost the properties that make the system trustworthy â€” and a monitoring system you can't trust is worse than no monitoring system at all.

---

## Staff-Level Differentiators

- Start with the **naive approach** and explain why it fails before proposing the real solution
- **Separate write/read/alert paths** â€” different characteristics, designed independently
- **Ground every decision in the scale numbers** from Step 1
- **Raise cardinality proactively** â€” don't wait to be asked
- **Name the catch-up problem** with capacity math (50% utilization headroom)
- **Distinguish slightly-late vs. very-late vs. out-of-order** â€” different solutions for each
- **Cover meta-monitoring** â€” the wrong answer is using the system to monitor itself
- **Connect to business reality** â€” Datadog's pricing model is a direct consequence of cardinality cost
