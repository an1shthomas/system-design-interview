# Google Docs â€” Collaborative Document Editor

**Problem:** Design a collaborative document editor like Google Docs.
**Difficulty:** Hard
**Core challenges:** Consistency under concurrent edits (OT/CRDTs) + stateful real-time connections at scale (WebSocket routing).

---

## Step 1: Clarifying Requirements & Scale

ğŸ¤ **Interviewer:** "Design a collaborative document editor like Google Docs. Where would you like to start?"

ğŸ‘¨â€ğŸ’» **Candidate:** "Before jumping in, let me ask a few questions to scope the problem correctly â€” this one has a lot of potential surface area."

**Functional scope:**

ğŸ‘¨â€ğŸ’» **Candidate:** "Are we supporting rich text formatting â€” bold, italics, images, tables â€” or can we assume plain text for now?"

ğŸ¤ **Interviewer:** "Assume a simple text editor."

ğŸ‘¨â€ğŸ’» **Candidate:** "Do we need to support document permissions â€” who can view vs. edit?"

ğŸ¤ **Interviewer:** "Out of scope."

ğŸ‘¨â€ğŸ’» **Candidate:** "What about document versioning â€” the ability to revert to previous states?"

ğŸ¤ **Interviewer:** "Out of scope for now, but good to flag as a potential deep dive."

ğŸ‘¨â€ğŸ’» **Candidate:** "Are we supporting comments and suggestions, or just direct edits?"

ğŸ¤ **Interviewer:** "Just direct edits."

ğŸ‘¨â€ğŸ’» **Candidate:** "Do we need offline editing â€” users making changes without a network connection?"

ğŸ¤ **Interviewer:** "Out of scope, but flag it."

**Collaboration scope:**

ğŸ‘¨â€ğŸ’» **Candidate:** "How many users can concurrently edit the same document?"

ğŸ¤ **Interviewer:** "Max 100 concurrent editors per document."

ğŸ‘¨â€ğŸ’» **Candidate:** "Do we need to show other users' cursor positions and presence?"

ğŸ¤ **Interviewer:** "Yes."

**Scale:**

ğŸ‘¨â€ğŸ’» **Candidate:** "What's the target scale â€” millions of users, billions of documents?"

ğŸ¤ **Interviewer:** "Yes, millions of concurrent users across billions of documents."

**Back-of-the-envelope (candidate does this out loud):**

> "Let me quickly size the problem."
> - Millions of concurrent users, each connected via a persistent WebSocket
> - A fast typist makes ~5 keystrokes/second â†’ ~5 edit operations/second per user
> - At 1M concurrent editors: ~5M edit operations/second globally
> - But these are spread across billions of documents â€” the key insight is that **contention is per-document, not global**. With max 100 editors per document, the per-document write rate is at most 500 ops/second â€” very manageable.
> - Each operation is small: ~50â€“100 bytes (operation type, position, character, timestamp, user ID)
> - Cursor updates are even more frequent but tiny (~20 bytes each)

"This shapes our design significantly. The global scale challenge is about managing millions of persistent WebSocket connections across a distributed fleet of servers. The per-document challenge is about consistency â€” ensuring 100 concurrent editors all converge on the same document state. **These are two separate problems and I'll tackle them separately.**"

**Key non-functional requirements:**
- **Eventual consistency** â€” all users must eventually see the same document state
- **Low latency** â€” edits should feel real-time, under 100ms round trip
- **Durability** â€” documents must survive server restarts (no data loss)
- **High availability** â€” the system must stay up even during failures
- **Scale** â€” millions of concurrent WebSocket connections, billions of documents

ğŸ¤ **Interviewer:** "You mentioned the 100 concurrent editor limit is interesting. Why does that matter to your design?"

ğŸ‘¨â€ğŸ’» **Candidate:** "It's a huge simplification. It means I don't need to worry about a single document generating massive throughput â€” 100 editors at 5 ops/second is 500 ops/second, which a single server can handle easily. More importantly, it means I can **route all editors of the same document to a single server** without that server becoming a hotspot. This makes consistency much simpler â€” one server has the authoritative view of the document and can serialize all edits. If we had 10,000 concurrent editors per document, we'd need a distributed consensus mechanism, which is a much harder problem. Google Docs made this exact design choice in production â€” beyond a certain number of users, new joiners are downgraded to read-only. That's a hint about their architecture."

> **âœ… What makes this staff-level:**
> - Immediately identifies the **two distinct challenges** â€” connection scaling vs. per-document consistency
> - Recognizes that the 100-editor cap is a **deliberate architectural constraint**, not just a product decision
> - Connects the back-of-the-envelope math to design consequences â€” per-document contention is manageable, global connection scaling is the hard part
> - Proactively flags out-of-scope items (offline mode, versioning) as potential deep dives

---

## Step 2: Core Entities & Data Modeling

ğŸ¤ **Interviewer:** "Walk me through the core entities in this system before we get into the architecture."

ğŸ‘¨â€ğŸ’» **Candidate:** "Let me identify the key nouns and think carefully about what data we need to store and what's ephemeral â€” that distinction will drive some important design decisions."

**1. Document** â€” stores metadata, not content:
```
Document {
  docId:      UUID        -- primary key
  title:      string
  ownerId:    userId
  createdAt:  timestamp
  updatedAt:  timestamp
  versionId:  UUID        -- points to current compacted version (important for deep dive)
}
```
"Notice I'm separating document metadata from document content. The metadata is small and queried frequently â€” it lives in Postgres. The actual content is represented as a sequence of operations â€” that lives in a separate store optimized for append-heavy writes."

**2. Operation (Edit)** â€” the atomic unit of change:
```
Operation {
  opId:        UUID
  docId:       UUID        -- which document
  userId:      UUID        -- who made the edit
  versionId:   UUID        -- which document version this op belongs to
  type:        enum        -- INSERT | DELETE
  position:    integer     -- where in the document
  content:     string      -- the character(s) inserted (null for DELETE)
  timestamp:   timestamp   -- set by the server, not the client
}
```
"A few important design decisions here. Timestamp is set by the **server**, not the client â€” this gives us a total ordering of operations which is essential for Operational Transformation. The `versionId` ties operations to a specific compacted snapshot â€” I'll explain why in the deep dive on storage. And operations are **append-only** â€” we never update or delete an operation record."

**3. Cursor / Presence** â€” ephemeral, in-memory only:
```
Cursor {
  docId:     UUID
  userId:    UUID
  position:  integer     -- character offset in document
  color:     string      -- UI color assigned to this user
  name:      string      -- display name
  updatedAt: timestamp
}
```
"Here's a critical observation: cursor and presence data is **ephemeral**. It only matters while a user is connected. We don't need to persist this to a database â€” it lives in the Document Service's memory, associated with the WebSocket connection."

**4. Editor (User)** â€” assume users are already authenticated; we just need their `userId`.

**Storage mapping:**

| Entity | Storage | Why |
|---|---|---|
| Document metadata | Postgres | Relational, flexible queries, low volume |
| Operations (edits) | Cassandra | Append-only, high write throughput, partition by docId |
| Cursor / Presence | In-memory (Document Service) | Ephemeral, tied to WebSocket connection lifetime |

ğŸ¤ **Interviewer:** "Why Cassandra for operations? Why not Postgres?"

ğŸ‘¨â€ğŸ’» **Candidate:** "Operations are append-only and partitioned naturally by docId. We'll query them as 'give me all operations for docId X after version Y, ordered by timestamp' â€” a simple range scan on a partition key. Cassandra is purpose-built for this: it partitions by docId so all operations for a document are co-located, orders by timestamp within a partition, and handles high append throughput without the write amplification that B-tree indexes cause in Postgres. We'd partition by `(docId, versionId)` and cluster by timestamp â€” that query pattern maps perfectly to Cassandra's data model."

ğŸ¤ **Interviewer:** "You said cursor data is ephemeral. What happens when a new user joins a document where others are already editing?"

ğŸ‘¨â€ğŸ’» **Candidate:** "When a new user connects via WebSocket, the Document Service has all active cursors in memory â€” one per connected user. It immediately sends the new user a snapshot of all current cursor positions as part of the connection handshake. Since all editors of the same document are connected to the **same** Document Service instance, this is a simple in-memory read â€” no database query needed. This is one of the key reasons we want all editors of the same document co-located on the same server."

> **âœ… What makes this staff-level:**
> - Separates metadata from content â€” not one monolithic document record
> - Critically **distinguishes persistent data** (operations) from **ephemeral data** (cursors) â€” and chooses appropriate storage for each
> - Justifies Cassandra over Postgres with a concrete query pattern argument
> - **Server-assigned timestamps** â€” a subtle but important correctness detail for OT
> - The `versionId` field foreshadows the compaction deep dive without getting lost in it yet

---

## Step 3: API Design

ğŸ¤ **Interviewer:** "Let's define the API. What interfaces does this system expose?"

ğŸ‘¨â€ğŸ’» **Candidate:** "This system has two distinct communication patterns that require different protocols â€” and choosing the right one for each is itself an important design decision."

"For document management â€” creating, listing documents â€” standard REST over HTTP is fine. These are low-frequency, stateless operations. But for collaborative editing, we need **bidirectional, real-time, persistent** communication. HTTP request-response doesn't work here â€” the server needs to push updates to clients without the client polling. **WebSockets** are the right choice: they give us a persistent full-duplex channel with low overhead per message."

**1. Document Management API (REST)**

```
POST /v1/docs
{ "title": "My Document" }
â†’ 201 Created { "docId": "doc_abc123", "title": "My Document", "createdAt": 1640000000 }

GET /v1/docs/{docId}
â†’ 200 OK { "docId": "doc_abc123", "versionId": "ver_xyz789", "updatedAt": 1640000000 }
```

"Note that `GET /docs/{docId}` returns metadata only â€” not the document content. The actual content is loaded over the WebSocket when the user opens the editor."

**2. Collaborative Editing API (WebSocket)**

```
WS /v1/docs/{docId}
Headers: Authorization: Bearer <token>
```

"On successful connection, the server immediately sends the client the current document state â€” the latest compacted snapshot plus any subsequent operations."

**Messages the client SENDS to server:**
```json
// Insert characters
{ "type": "insert", "opId": "op_client_123", "position": 5, "content": ", world", "clientTimestamp": 1640000000 }

// Delete characters
{ "type": "delete", "opId": "op_client_124", "position": 5, "length": 3 }

// Update cursor position
{ "type": "updateCursor", "position": 7 }
```

**Messages the client RECEIVES from server:**
```json
// Initial document state on connection
{ "type": "init", "versionId": "ver_xyz789", "content": "Hello",
  "cursors": [{ "userId": "u1", "position": 3, "color": "#FF5733", "name": "Alice" }] }

// A transformed operation from another editor
{ "type": "operation", "opId": "op_server_456", "userId": "u1",
  "operationType": "insert", "position": 5, "content": ", world", "serverTimestamp": 1640000001 }

// Acknowledgment of client's own operation
{ "type": "ack", "opId": "op_client_123", "serverTimestamp": 1640000001 }

// Another user's cursor moved
{ "type": "cursorUpdate", "userId": "u1", "position": 7 }

// A user joined or left
{ "type": "presenceUpdate", "userId": "u1", "status": "joined" }
```

ğŸ¤ **Interviewer:** "Why does the client send its own opId? And why does the server echo it back in the ack?"

ğŸ‘¨â€ğŸ’» **Candidate:** "Two reasons. First, **idempotency** â€” if the client sends an operation but the network drops before receiving the ack, it will retry. The server uses the client-generated opId to detect and deduplicate the retry rather than applying the same edit twice. Second, **optimistic UI** â€” the client applies its own edit immediately to the local document without waiting for the server (otherwise the editor would feel laggy at 100ms+ round trip). When the ack arrives with the server timestamp, the client knows the operation has been officially ordered and can reconcile its local state."

ğŸ¤ **Interviewer:** "Why not use Server-Sent Events (SSE) instead of WebSockets?"

ğŸ‘¨â€ğŸ’» **Candidate:** "SSE is unidirectional â€” server to client only. For a collaborative editor, clients need to send edits to the server as well. You could technically combine SSE for receiving with HTTP POST for sending, but that's two separate connections per user, more complex state management, and higher overhead per operation. WebSockets give us a single persistent full-duplex channel â€” simpler, lower latency, and the right tool for this use case. SSE makes more sense for read-heavy, one-way push scenarios like live sports scores or notification feeds."

> **âœ… What makes this staff-level:**
> - Justifies the **protocol choice** (WebSocket vs REST vs SSE) with concrete reasoning
> - Designs the `init` message carefully â€” document state + all cursor positions sent on connection
> - **Client-generated opId** for idempotency and optimistic UI â€” a subtle but production-critical detail
> - Server timestamps for authoritative ordering â€” connects back to the OT requirement from entities
> - Separates `ack` messages from broadcast messages â€” client needs to know its own op was accepted

---

## Step 4: High-Level Architecture

ğŸ¤ **Interviewer:** "Walk me through the high-level architecture. How does the system work end to end?"

ğŸ‘¨â€ğŸ’» **Candidate:** "I'll build this up incrementally â€” starting with document creation, then the collaborative editing write path, then the read path, and finally cursor/presence. I'll flag scaling concerns as I go and save them for deep dives."

**Components:**
- **API Gateway** â€” handles auth, routing, rate limiting
- **Document Metadata Service** â€” CRUD for document metadata, backed by Postgres
- **Document Service** â€” the stateful core: manages WebSocket connections, applies OT, broadcasts updates. **One instance owns all connections for a given document.**
- **Document Operations DB** â€” Cassandra, append-only log of operations partitioned by docId
- **Document Metadata DB** â€” Postgres, document metadata

**1. Creating a Document (simple)**

```
Client â†’ API Gateway â†’ Document Metadata Service â†’ Postgres
                                â†“
                         { docId: "doc_abc123" }
```

"Standard horizontally-scaled stateless CRUD service. I won't dwell here â€” the interesting parts are ahead."

**2. Collaborative Editing â€” Write Path**

*Step 1 â€” Connection:* Client opens a WebSocket to the Document Service responsible for that docId. (Routing mechanism deferred to deep dive.)

*Step 2 â€” Document Load:* On connection, the Document Service:
1. Fetches the current `versionId` from Postgres
2. Loads the compacted snapshot + all subsequent operations from Cassandra
3. Reconstructs the current document state in memory
4. Sends the `init` message to the client

*Step 3 â€” Receiving an Edit:*
```
Client A --[insert op]--> Document Service
```
The Document Service:
1. Assigns a **server timestamp** â€” authoritative ordering for OT
2. Applies **Operational Transformation** against any concurrent ops already applied
3. Writes the transformed op to Cassandra â€” **durability guaranteed before ack**
4. Sends `ack` to Client A
5. Broadcasts the transformed op to all other connected clients (B, C, etc.)

```
Document Service â†’ Cassandra (write op)
                â†’ Client A (ack)
                â†’ Client B, C... (broadcast transformed op)
```

ğŸ¤ **Interviewer:** "Why write to Cassandra before sending the ack? Why not ack immediately and write asynchronously?"

ğŸ‘¨â€ğŸ’» **Candidate:** "Durability. If we ack the client and then crash before writing to Cassandra, that operation is lost forever â€” the client thinks it was saved, but it wasn't. The user could close the browser, the document would reload from Cassandra, and their edit is silently gone. That's a catastrophic user experience. By writing to Cassandra first, we guarantee the operation survives any server crash. This is essentially **write-ahead logging** applied to collaborative editing."

**3. Read Path â€” Viewing Changes in Real-Time**

"The read path is simple precisely because of our design choice to co-locate all editors on the same Document Service instance."

When Client A's edit is processed:
- The Document Service has all connected WebSocket handles in memory
- It simply iterates over connected clients for that docId and sends the transformed op to each one
- **No pub/sub, no message broker, no cross-server communication needed**

"This is the key payoff of the co-location design. Broadcasting to 100 concurrent editors is just 100 in-memory WebSocket writes â€” trivially fast, no network hops."

**4. Cursor & Presence**

- Client sends `updateCursor` message â†’ Document Service updates its in-memory cursor map â†’ broadcasts `cursorUpdate` to all other connected clients
- On disconnect: Document Service removes cursor from in-memory map â†’ broadcasts `presenceUpdate` with `status: "left"`
- **None of this touches Cassandra or Postgres.** Cursor state is entirely in-memory.

**Full architecture:**
```
[Client A] â†â”€â”€WebSocketâ”€â”€â†’ â”
[Client B] â†â”€â”€WebSocketâ”€â”€â†’ â”œâ”€ [Document Service]  â†â†’ [Cassandra: Operations DB]
[Client C] â†â”€â”€WebSocketâ”€â”€â†’ â”˜         â†•
                                [Postgres: Metadata DB]

[Client D] â”€â”€RESTâ”€â”€â†’ [API Gateway] â†’ [Document Metadata Service] â†’ [Postgres]
```

ğŸ¤ **Interviewer:** "What happens if the Document Service crashes while editors are connected?"

ğŸ‘¨â€ğŸ’» **Candidate:** "Two things happen. First, all connected WebSocket clients detect the connection drop and enter a reconnect loop with exponential backoff. Second, they reconnect to a new Document Service instance. Because all operations were durably written to Cassandra before being acked, the new instance can reconstruct the full document state by replaying operations from Cassandra. Any operations the client had sent but not yet received acks for get retried â€” the client-generated opId ensures they're deduplicated. The in-memory cursor state is lost but clients re-broadcast their cursor positions on reconnect. We lose ephemeral state (cursors momentarily) but **never persistent state** (document content)."

> **âœ… What makes this staff-level:**
> - Builds up incrementally â€” document creation â†’ write path â†’ read path â†’ presence, each with clear rationale
> - **Write-to-Cassandra before ack** â€” explicitly connects this to durability and the write-ahead log pattern
> - **Co-location as a first-class design choice** â€” not an afterthought, but the reason the read path is simple
> - Clearly separates ephemeral cursor state from durable operation state
> - Handles crash recovery with a concrete answer: replay from Cassandra + client retry with opId dedup

---

## Deep Dive 1: Collaborative Editing â€” OT vs. CRDTs

ğŸ¤ **Interviewer:** "You mentioned Operational Transformation. Walk me through why it's needed and how it actually works. And how does it compare to CRDTs?"

ğŸ‘¨â€ğŸ’» **Candidate:** "Let me build up to OT by showing why naive approaches fail, then explain OT and CRDTs as two different philosophies for solving the same problem."

**âŒ Naive Approach 1: Send Full Document Snapshots**

Why it fails:
- Extremely inefficient â€” a 50KB document sends 50KB on every keystroke
- **Lost updates**: If User A and User B both edit and send their snapshots simultaneously, whoever arrives last wins â€” the other's changes are silently overwritten

**âŒ Naive Approach 2: Send Operations (No Transformation)**

Concrete failure example:
```
Document starts as: Hello!
Position:  0 1 2 3 4 5
Character: H e l l o !

User A inserts ", world" after position 4 â†’ sends INSERT(5, ", world")
User B deletes "!" at position 5       â†’ sends DELETE(5)

Both send at the same time. Server applies User A's op first:
  Hello, world!
  0123456789012  â† "!" is now at position 12, not 5

Now User B's DELETE(5) arrives â€” it deletes "," instead of "!":
  Hello world!  â† WRONG! Comma deleted, "!" survived
```

"The problem: User B's operation was based on the document state they saw locally. When concurrent operations arrive out of order, positional references become stale. This is the **core consistency problem**."

**âœ… Good Solution: Operational Transformation (OT)**

Core idea: Transform each incoming operation based on operations that have already been applied, so its **intent is preserved** regardless of ordering.

Same example with OT:
```
User B's DELETE(5) arrives after User A's INSERT(5, ", world") has already been applied.
The server knows:
  - User B's op was based on a state where "!" was at position 5
  - User A's INSERT added 7 characters before position 5+
  - Therefore, "!" is now at position 5 + 7 = 12

The server transforms User B's op: DELETE(5) â†’ DELETE(12)
Result: Hello, world  â† correct!
```

How OT works at the server â€” when a new operation arrives:
1. Determine which operations the client had already seen (via the client's last-known version)
2. Transform the incoming op against all ops applied since then
3. Apply the transformed op and append to the log
4. Broadcast the transformed op to other clients

OT also happens on the client: when Client B receives a server broadcast, it transforms it against any local unacknowledged ops before applying. This ensures local optimistic updates stay consistent with incoming server ops.

```
Server: [Ea, Eb_transformed]
Client A sees: Ea â†’ Eb_transformed  âœ…
Client B sees: Eb (local) â†’ Ea_transformed  âœ…
Both converge to same document âœ…
```

**OT trade-offs:**
- âœ… Low memory â€” no need to keep tombstones, just the operation log
- âœ… Efficient â€” works well for text specifically
- âœ… Battle-tested â€” Google Docs, Notion, and most production editors use it
- âŒ Requires a central server to establish authoritative op ordering â€” can't be fully peer-to-peer
- âŒ Tricky to implement correctly â€” transformation functions must handle all op combinations

---

**âœ… Alternative: Conflict-free Replicated Data Types (CRDTs)**

Core idea: Instead of transforming operations, design the data structure so all operations are **commutative** â€” they produce the same result regardless of order.

**How CRDTs handle positions:**

Instead of integer positions (which shift when characters are inserted/deleted), CRDTs assign each character a **globally unique, stable fractional position** â€” like a real number between 0 and 1 that never changes.

```
Document: H(0.1)  e(0.2)  l(0.3)  l(0.4)  o(0.5)  !(0.6)

User A inserts ", world" â†’ assigns it position 0.53
User B deletes "!" at 0.6

Both ops can be applied in any order:
  - DELETE(0.6) removes the "!" regardless of when applied
  - INSERT(0.53, ", world") lands between "o" and "!" regardless

Result: H e l l o ,   w o r l d
        0.1 0.2 0.3 0.4 0.5 0.53 [0.6 is tombstoned]
```

**Deletions use tombstones** â€” deleted characters are marked as deleted but not removed from the list. This prevents position conflicts when two users both try to delete the same character:
- User A deletes position 0.5 â†’ marks it as tombstone
- User B also deletes position 0.5 â†’ finds it already tombstoned â†’ no-op âœ…

**CRDT trade-offs:**

| | OT | CRDT |
|---|---|---|
| Central server | Required (for ordering) | Not required |
| Memory | Low (no tombstones) | Higher (tombstones accumulate) |
| Offline support | Hard | Natural |
| Peer-to-peer | âŒ | âœ… |
| Implementation complexity | High (transform functions) | High (position allocation, GC) |
| Best for | Online collaborative editing with server | Offline-first, P2P scenarios |

**When to choose CRDTs:** Offline-first applications (Notion offline mode), P2P sync (local-first software like Obsidian), or scenarios where network partitions are expected.

**When to choose OT:** Online collaborative editors with a server (Google Docs, Notion online) where you can guarantee a single authoritative ordering point.

---

### ğŸ“š Supplementary: CRDTs in Depth

#### 1. Fractional ID Generation in Practice

**The problem with naive fractional IDs:**

*Problem 1 â€” Collisions:* If User A and User B both insert between positions 0.5 and 0.6 and both randomly pick 0.53, they've generated the same ID for two different characters. The document becomes ambiguous.

*Problem 2 â€” Precision exhaustion:* If users repeatedly insert between the same two characters, IDs grow longer and longer:
```
0.5 â†’ 0.51 â†’ 0.511 â†’ 0.5111 â†’ 0.51111...
```
After thousands of inserts in the same spot, IDs become arbitrarily long â€” consuming huge memory and slowing down sorting.

**Solution to collisions â€” Site ID tie-breaking:**

Each client is assigned a globally unique site ID (e.g., a UUID). Every position is a tuple:
```
Position = (fractional_number, site_id, sequence_number)
```
Comparison is lexicographic: fractional number first, then site_id, then sequence_number. Two clients can independently pick the same fractional number and still produce a total, unambiguous ordering.
```
User A inserts at: (0.53, "site-A", 42)
User B inserts at: (0.53, "site-B", 17)
â†’ site-A < site-B alphabetically â†’ User A's character comes first âœ…
```

**Solution to precision exhaustion â€” LSEQ (tree-based position allocation):**

Instead of simple decimals, production CRDTs use a tree structure:
- Each level of the tree has a fixed number of slots (e.g., 32 at level 1, 1024 at level 2, etc.)
- A position is a path from root to leaf: e.g., `[15, 7, 22]`
- To insert between two positions, go one level deeper and pick a slot in between

```
Level 1:  [0 ............ 15 ............. 31]
                           â†“
Level 2:          [0 .... 7 .......... 31]
                           â†“
Level 3:      [0 .. 3 .. 7 .. 14 .. 31]
```

Positions grow logarithmically, not unboundedly. LSEQ alternates between two strategies at each tree level:
- **Boundary+**: allocate near the left boundary (good for left-to-right typing)
- **Boundary-**: allocate near the right boundary (good for right-to-left typing)

This heuristic dramatically reduces ID length for common editing patterns.

---

#### 2. Garbage Collection of Tombstones

**The problem:** Tombstones accumulate forever. In a long-lived document with 100,000 characters typed and 80,000 deleted, the CRDT stores all 100,000 entries (80,000 tombstones + 20,000 visible).

**The safe GC condition:** A tombstone can only be removed when every client that will ever interact with this document has already seen and applied the deletion. Otherwise, a client coming back online might try to insert relative to a position that no longer exists in the data structure.

**Method 1 â€” Version Vectors:**

Each client maintains a version vector â€” a map of `{siteId â†’ last_sequence_number_seen}`:
```
User A's version vector: { "site-A": 42, "site-B": 38, "site-C": 15 }
```
A tombstone for operation `(site-B, seq=30)` can be GC'd when all currently connected clients show `site-B â‰¥ 30`. The server periodically broadcasts version vectors; when all vectors confirm they've seen a deletion, the tombstone is purged.

**Method 2 â€” Epoch-based GC (used by Yjs):**

1. The server periodically takes a snapshot of the current document state (visible text only, no tombstones), assigned an epoch number
2. New clients joining after this epoch load from the snapshot, not from scratch
3. Tombstones from before the snapshot epoch are safe to delete â€” no new client will ever need to reference them

```
Epoch 1: [snapshot of document at t=0]
    â†“ new ops accumulate...
Epoch 2: [snapshot of document at t=1hr]
    â†’ tombstones from Epoch 1 can now be GC'd
```

This is the same compaction concept as the Document Service's online compaction â€” just applied to CRDTs.

**Method 3 â€” Causal Stability (academic):**

An operation is causally stable when it's been seen by all sites and no future operation can possibly causally precede it. Once causally stable, its tombstone is safe to collect. Provably correct but requires sophisticated vector clock tracking.

---

#### 3. How Yjs Works â€” A Real Production CRDT

Yjs is the most widely used open-source CRDT library (used in production by many collaborative tools including Notion's offline mode).

**Core data structure â€” doubly linked list of Items:**
```
Item {
  id:        (clientID, clock)    â† unique ID
  content:   string               â† the character(s)
  deleted:   boolean              â† tombstone flag
  left:      Item                 â† left neighbor at time of insertion
  right:     Item                 â† right neighbor at time of insertion
  origin:    Item                 â† left neighbor WHEN THIS ITEM WAS INSERTED
}
```
The `origin` field is crucial â€” it records what was to the left of this item **at the time it was inserted**, not the current left neighbor. This is how Yjs resolves concurrent insertions at the same position deterministically.

**Yjs insertion algorithm (YATA):**

When User A inserts character X between items L and R:
```
Item X { origin: L, right: R }
```
When this op arrives at another client, Yjs finds where to place X using this rule:
1. Start from `L.right` (the item immediately right of X's left origin)
2. Walk right through items until finding the correct position
3. For concurrent insertions at the same spot: items with an origin further to the left go further right; if origins are equal, break ties by clientID (consistent across all clients)

This guarantees all clients place the item in the same position, even if they receive ops in different orders. **No server needed for ordering.**

**Yjs awareness protocol:**

For ephemeral state (cursor positions, user presence, selection ranges) â€” kept completely separate from the document CRDT because:
- Cursor state doesn't need to be persisted
- It doesn't need tombstones or version vectors
- It can be lossy â€” missing one cursor update is fine

Uses a simple last-write-wins map per client:
```json
{
  "client-A": { "cursor": 42, "name": "Alice", "color": "#FF5733", "timestamp": 1640000001 },
  "client-B": { "cursor": 17, "name": "Bob",   "color": "#33FF57", "timestamp": 1640000002 }
}
```
Each client owns its own entry. Conflicts are impossible â€” each client only writes to its own entry. This is exactly the in-memory cursor map from the Google Docs architecture â€” Yjs just formalizes it as part of the protocol.

**Yjs state vectors for efficient sync:**

```
Client A state: { A: 50, B: 30 }
Client B state: { A: 45, B: 35 }
â†’ A needs to send ops: B[31..35]
â†’ B needs to send ops: A[46..50]
```
When two clients connect (or reconnect after offline), they exchange state vectors and each sends only the missing ops â€” no full sync needed.

**CRDT internals summary:**

| Concept | Problem it solves | How |
|---|---|---|
| Fractional IDs (LSEQ) | Stable positions despite inserts/deletes | Tree-based position allocation |
| Site ID tie-breaking | Collision-free concurrent inserts | Lexicographic comparison of (position, siteID) |
| Tombstones | Safe concurrent deletes | Never remove, just mark deleted |
| Version vectors | Know what each client has seen | Map of {siteId â†’ last_seq_seen} |
| Garbage collection | Reclaim tombstone memory | Delete tombstones seen by all clients |
| Yjs YATA algorithm | Deterministic concurrent insert ordering | Origin-based placement rule |
| Yjs awareness | Ephemeral cursor/presence | Last-write-wins per-client map |
| Yjs state vectors | Efficient reconnect sync | Exchange vectors, send only missing ops |

---

## Deep Dive 2: Scaling WebSocket Connections to Millions of Users

ğŸ¤ **Interviewer:** "Your current design has all editors of a document on a single Document Service instance. How do you scale this to millions of concurrent WebSocket connections?"

ğŸ‘¨â€ğŸ’» **Candidate:** "This is the hardest scaling challenge in this system. Let me explain why it's tricky before proposing solutions."

**Why this is hard:**

"WebSocket connections are stateful and long-lived. Unlike HTTP where any server can handle any request, a WebSocket connection is pinned to a specific server for its entire lifetime. This creates two tensions:"

1. **Connection scaling:** A single server can handle ~50,000â€“100,000 concurrent WebSocket connections. At 1M concurrent editors we need at least 10â€“20 servers.
2. **Co-location requirement:** For OT to work correctly, all editors of the same document must be on the same server. We can't have User A on Server 1 and User B on Server 2 editing the same document â€” they'd each apply OT independently with no shared ordering, causing divergence.

"So we need horizontal scaling, but with a constraint: **same document â†’ same server**. This is a classic consistent routing problem."

**âŒ Bad Solution: Simple Round-Robin Load Balancing**

"A standard load balancer distributes connections round-robin across servers. User A might land on Server 1, User B on Server 2. Now we have two servers with no shared document state, applying OT independently â€” classic split-brain. The two users will see diverging documents. This fundamentally breaks correctness."

**âœ… Good Solution: Consistent Hash Ring**

Route each document to a deterministic server by hashing the docId. All editors of the same document always hash to the same server â€” co-location guaranteed.

How it works:
1. Document Service instances join a consistent hash ring, each owning a range of hash values
2. When a client wants to connect for `doc_abc123`, it first makes an HTTP request to any server
3. That server computes `hash(doc_abc123)` â†’ determines which server owns that hash range
4. If it's not the current server, it responds with a redirect to the correct server
5. The client reconnects directly to the correct server and upgrades to WebSocket

**ZooKeeper for ring coordination:**
- Each Document Service instance registers itself in ZooKeeper on startup
- ZooKeeper maintains the current ring configuration
- All servers watch ZooKeeper for ring changes
- Clients don't talk to ZooKeeper directly â€” only servers do

```
Client â†’ any Server â†’ check ZooKeeper hash ring
                    â†’ redirect to correct Server
                    â†’ WebSocket connection established
```

"Why consistent hashing specifically? When we add or remove a server, consistent hashing minimizes disruption. A standard hash (`docId % N`) would remap ~50% of all documents when N changes. **Consistent hashing remaps only ~1/N documents** â€” only those whose hash range now belongs to the new server."

ğŸ¤ **Interviewer:** "What happens when a server fails or a new server is added?"

ğŸ‘¨â€ğŸ’» **Candidate:**

**Server failure:**
1. ZooKeeper detects the failed server via heartbeat timeout (typically 3â€“10 seconds)
2. ZooKeeper removes the server from the ring and notifies all remaining servers
3. The failed server's hash ranges are redistributed to neighboring servers
4. Clients detect the WebSocket drop and enter a reconnect loop
5. On reconnect, they hit any live server, which redirects them to the new owner
6. The new owner loads document state from Cassandra and resumes

"No data is lost â€” all operations were written to Cassandra before being acked. The reconnect is seamless from a data perspective."

**Adding a new server (scaling up):**
1. New server joins the ring and registers with ZooKeeper
2. It takes ownership of a portion of the hash range from its neighbors
3. Affected clients are disconnected and redirected to the new server
4. During transition, ZooKeeper's linearizable writes ensure only one server is authoritative at any time

"The hairiest part is the migration protocol: the old server **stops accepting new ops for migrating documents** and drains in-flight ops to Cassandra before handing off. The new server only starts accepting connections after confirming the handoff is complete via a ZooKeeper transaction."

ğŸ¤ **Interviewer:** "That's complex. Is there a simpler alternative for scaling WebSocket connections?"

ğŸ‘¨â€ğŸ’» **Candidate:** "Yes â€” and it's worth discussing the trade-off."

**Alternative: WebSocket Gateway + Redis Pub/Sub**
```
[Client A] â”€â”€WebSocketâ”€â”€â†’ [WS Gateway 1] â”€â”€subscribe(docId)â”€â”€â†’ [Redis Pub/Sub]
[Client B] â”€â”€WebSocketâ”€â”€â†’ [WS Gateway 2] â”€â”€subscribe(docId)â”€â”€â†’ [Redis Pub/Sub]
                                                                       â†‘
                                          [Document Service] â”€â”€publish(docId, op)â”€â”€â”˜
```
- WebSocket Gateways are stateless â€” any gateway can accept any client
- Document Service is stateless â€” it processes ops and publishes results to Redis
- Redis Pub/Sub fans out to all gateways subscribed to that docId

Why this is simpler: No consistent hashing, no ZooKeeper, no connection migration.

Why we still choose consistent hashing: "The pub/sub approach **breaks our OT requirement**. OT needs a single authoritative server to establish a total ordering of operations for each document. If two Document Service instances both process ops for the same document concurrently, they'd produce different orderings and clients would diverge. You'd need a distributed lock or consensus mechanism â€” which adds back the complexity we tried to avoid. **The pub/sub approach works well for simpler broadcast problems** â€” live comments, presence-only â€” where there's no ordering dependency. For OT-based collaborative editing, we need the centralized ordering that consistent hashing gives us."

ğŸ¤ **Interviewer:** "What about memory? With millions of documents, can you keep them all in memory?"

ğŸ‘¨â€ğŸ’» **Candidate:** "No â€” and we shouldn't try. We only keep **active documents** in memory â€” those with at least one connected editor. When the last editor disconnects, we evict the document from memory. Let me size this: at 1M concurrent editors across at most 100 editors per document, we have at least 10,000 active documents. At 50KB average memory per active document, that's 500MB of active document state per server â€” very manageable. The key is aggressive eviction of idle documents."

> **âœ… What makes this staff-level:**
> - Names the exact tension â€” **horizontal scaling vs. co-location requirement** â€” before proposing solutions
> - Explains why **consistent hashing over simple hashing** â€” minimizes remapping on topology changes
> - Addresses failure and scaling events with a concrete migration protocol
> - Honestly evaluates the **pub/sub alternative** â€” knows when it works and why it doesn't fit here
> - Sizes the memory problem â€” does the math rather than just saying "evict inactive documents"

---

## Deep Dive 3: Storage Compaction & Snapshots

ğŸ¤ **Interviewer:** "You're storing every edit operation forever in Cassandra. A popular document could have millions of operations. What problems does this cause and how do you solve it?"

ğŸ‘¨â€ğŸ’» **Candidate:** "Great question â€” this is a problem that grows silently and bites you in production."

**Why unbounded operation storage is a problem:**

*Problem 1: Cassandra storage growth*
- At 5M ops/second globally, each op ~100 bytes â†’ **500MB/second** of raw operation data
- Per day: **~43TB** of operation logs
- Unlike the metrics system where old data ages out naturally, we can't just delete old operations â€” they're needed to reconstruct the document.

*Problem 2: Document load time*
- When a new editor connects to a document with 1 million operations, the Document Service must fetch all 1M ops from Cassandra and replay them sequentially
- At 10,000 ops/second replay speed, that's **100 seconds** of load time â€” completely unacceptable

*Problem 3: Memory pressure*
- Keeping a million operations in memory per active document quickly exhausts the Document Service's heap

**The solution: Operation Compaction (Snapshotting)**

"Periodically collapse many operations into a single snapshot representing the document's full current state. Instead of storing 1M individual edits, store one `INSERT(0, "full document text")` operation. This is directly analogous to **database checkpointing** in WAL-based systems like Postgres."

**The versionId mechanism** â€” this is why the `versionId` field exists:

```
Document { docId, versionId: "ver_002", ... }

Operations:
(docId, versionId="ver_001", t=1) INSERT(0, "Hello")         â† old version
(docId, versionId="ver_001", t=2) INSERT(5, ", world")       â† old version
(docId, versionId="ver_002", t=3) INSERT(0, "Hello, world")  â† compacted snapshot
(docId, versionId="ver_002", t=4) INSERT(12, "!")            â† new ops after snapshot
```

When loading a document, the Document Service:
1. Reads the current `versionId` from Postgres
2. Fetches only operations with that `versionId` from Cassandra
3. Applies them in order to reconstruct the document

Old operations under previous versionIds can be safely deleted â€” they're no longer referenced. "The `versionId` is the atomic switch between old and new compacted state."

**âŒ Option 1: Offline Compaction Service**

A separate background job that:
1. Identifies documents that are large, haven't been compacted recently, and are currently inactive
2. Reads all operations for the current versionId from Cassandra
3. Replays them to reconstruct the document text
4. Writes a single `INSERT(0, fullText)` under a new versionId
5. Atomically flips the versionId in Postgres via compare-and-swap:
   ```sql
   UPDATE docs SET versionId="ver_002" WHERE docId=X AND versionId="ver_001"
   ```
6. Schedules deletion of old operations

"The **compare-and-swap is critical**: ensures we only flip if nothing else has changed the versionId concurrently. If a Document Service instance started a compaction at the same time, one will lose the CAS and retry."

Challenges:
- Must verify the document is truly inactive before compacting â€” otherwise racing with live edits
- Compaction Service is a heavy Cassandra reader â€” needs rate limiting to avoid impacting live writes
- Deletion of old ops should be deferred â€” immediate deletion risks a race where a client loaded the old versionId but hasn't finished replaying yet

**âœ… Option 2: Online Compaction by the Document Service (preferred)**

"The offline Compaction Service has a fundamental awkwardness: it needs to coordinate with the Document Service to check if a document is active. What if we eliminate that coordination by **having the Document Service do compaction itself**?"

When does it compact? **When the last editor disconnects** â€” the Document Service already knows the document is now idle, and it has the full document state in memory â€” no need to replay from Cassandra.

```
Last client disconnects
        â†“
Document Service (async, low priority):
  â†’ Serialize current in-memory document state â†’ fullText
  â†’ Write INSERT(0, fullText) to Cassandra under new versionId
  â†’ CAS flip versionId in Postgres
  â†’ Schedule old op deletion
  â†’ Evict document from memory
```

Why this is better:
- **No coordination needed** â€” Document Service inherently knows when a document is idle
- **Already have the state in memory** â€” no need to replay from Cassandra
- **Natural timing** â€” documents are always compact when inactive; next editor loads instantly
- **Simpler architecture** â€” one fewer service to deploy and operate

The low-priority process detail: "We offload compaction to a separate OS process with lower CPU scheduling priority (via `nice` on Linux). This prevents a large document compaction from consuming CPU that a simultaneously connecting new editor needs."

**Edge cases:**
- *Server crash during compaction:* Old versionId is still in Postgres â€” the next load replays from Cassandra as normal. Compaction simply didn't happen â€” correctness preserved.
- *New editor connects while compaction is running:* Document Service serves from in-memory state. Compaction finishes independently and doesn't affect already-connected editors.
- *Very large documents:* Serialization might take a few seconds. Fine since it's async and low-priority.

ğŸ¤ **Interviewer:** "How would you extend this to support document versioning â€” letting users revert to earlier versions?"

ğŸ‘¨â€ğŸ’» **Candidate:** "Elegant extension â€” our current design almost supports it for free. Instead of deleting old versionIds after compaction, we **keep them**. Each versionId becomes a named checkpoint:"

```
DocumentVersions {
  docId:       UUID
  versionId:   UUID
  createdAt:   timestamp
  label:       string    â† optional user-assigned label e.g. "Before major rewrite"
  compactedOp: UUID      â† points to the snapshot operation in Cassandra
}
```

"To revert to a previous version, we replay from that version's compacted snapshot. We could apply tiered retention: full granularity for 30 days, then hourly snapshots, then daily snapshots for older history. This is exactly how Google Docs version history works."

ğŸ¤ **Interviewer:** "What about memory optimization for the Document Service?"

ğŸ‘¨â€ğŸ’» **Candidate:** "Three levers:
1. **Aggressive eviction on last disconnect** â€” evict immediately after compaction. No idle documents in memory.
2. **LRU eviction for recently accessed documents** â€” if a viewer disconnected recently, keep briefly in case they return.
3. **Operation log trimming in memory** â€” once an op has been acked and broadcast, it only needs to stay in memory for OT transformation against new incoming ops. We bound the in-memory operation history to a rolling window (e.g., last 1,000 ops) â€” very old ops are unlikely to be needed for transformation against new edits.

With these three, the memory footprint per active document is bounded to: document text size + last N operations buffer + cursor map â€” well under 1MB for a typical document."

> **âœ… What makes this staff-level:**
> - **Sizes the problem** â€” 43TB/day and 100-second load times make the urgency concrete
> - Connects to **WAL/checkpointing** â€” shows understanding of how databases solve the same problem
> - The **versionId mechanism** â€” a clean, atomic solution with no distributed locks needed
> - Compares offline vs. online compaction â€” and **picks online with clear reasoning**
> - Handles edge cases â€” crash during compaction, new editor during compaction
> - Extends to versioning naturally â€” shows the design was forward-thinking

---

## Step 6: Wrap-up & Trade-offs

ğŸ¤ **Interviewer:** "We're coming up on time. Summarize your design, the key trade-offs, and what you'd revisit given more time."

ğŸ‘¨â€ğŸ’» **Candidate:** "Let me zoom out and give you the full picture, then walk through the deliberate trade-offs."

**Full System Summary:**

*1. Document Management (stateless, simple):*
```
Client â†’ API Gateway â†’ Document Metadata Service â†’ Postgres
```
Standard horizontally-scaled CRUD. Deliberately kept separate from the collaborative editing layer.

*2. Collaborative Editing (stateful, complex):*
```
Client A, B, C â†â”€â”€WebSocketâ”€â”€â†’ Document Service
                                (owns docId via consistent hash ring + ZooKeeper)
                                        â†•
                                [Cassandra: operation log]
```
The Document Service is the authoritative OT server for each document. It holds all active WebSocket connections, assigns server timestamps, applies OT, broadcasts transformed ops, and keeps cursor/presence state in memory.

*3. Storage Lifecycle:*
```
Document Service (on last disconnect)
  â†’ compacts ops
  â†’ writes snapshot to Cassandra
  â†’ CAS flips versionId in Postgres
  â†’ evicts from memory
```

**Key Trade-offs Made:**

| Trade-off | Chose | Rationale |
|---|---|---|
| OT vs CRDTs | OT | Centralized architecture, lower memory, battle-tested. CRDTs better for offline-first/P2P. |
| Consistent hash ring vs pub/sub gateway | Hash ring | OT correctness requires single authoritative server per document. Pub/sub would need distributed locking. |
| Online vs offline compaction | Online (Document Service) | Eliminates cross-service coordination, leverages already-in-memory state, simpler architecture. |
| Cassandra vs Postgres for operations | Cassandra | Append-only, partitioned by docId, simple range queries â€” perfect Cassandra use case. |
| Server-assigned vs client timestamps | Server | Client clocks are unreliable. OT requires total ordering â€” server provides single authoritative clock. |

**What I'd Do Differently Given More Time:**

1. **Read-only mode for large audiences** â€” viewers don't need WebSocket connections to the Document Service. A separate read-only path using stateless Read Gateway servers subscribing to document update streams via Redis pub/sub would decouple viewer scale from editor scale completely.

2. **Offline editing support** â€” with OT, offline is hard (must transform local ops against the server's full op history since disconnect). CRDTs handle this more elegantly. Consider a hybrid: OT for online, CRDT-based sync for offline reconnect (Yjs's awareness protocol is a good reference).

3. **Rich text beyond plain text** â€” the OT transformation functions become significantly more complex for structured content. The industry has converged on document models like ProseMirror's schema or Quill's Delta format â€” I'd adopt one rather than inventing my own.

4. **Throttled cursor updates** â€” cursor positions need to be transformed when characters are inserted before a cursor. For very large documents, transmitting 100 cursors on every keystroke has UI performance implications â€” throttle cursor updates to ~100ms intervals rather than on every keystroke.

5. **Multi-region deployment** â€” our design is single-region. Multi-region collaborative editing is extremely hard with OT's ordering guarantee. Practical approaches: geo-routing editors to the nearest region with replication lag for reads, or accepting higher latency for inter-region collaboration.

ğŸ¤ **Interviewer:** "Strong design overall. Thank you."

**ğŸ¯ Final Reflection**

"The core insight of this design is that Google Docs is actually two very different problems that share a storage layer:

**The first is a consistency problem** â€” how do you make concurrent edits from multiple users converge to the same document? OT solves this elegantly by transforming operations relative to each other, with the Document Service as the authoritative ordering point. This is fundamentally a single-threaded problem per document â€” and we lean into that rather than fighting it.

**The second is a connection scaling problem** â€” how do you maintain millions of stateful WebSocket connections while preserving the co-location constraint that OT requires? Consistent hashing with ZooKeeper solves this by making routing deterministic â€” every server always knows which server owns any given document.

The design deliberately keeps these two concerns separate. The OT logic doesn't care how we route connections. The routing logic doesn't care how OT works. **That separation of concerns is what makes the system comprehensible and maintainable as it scales.**"

---

**Overall Assessment:**

| Area | Quality | Notes |
|---|---|---|
| Requirements clarification | âœ… Strong | Identified two distinct challenges immediately |
| Core entities | âœ… Strong | Persistent vs. ephemeral data distinction |
| API design | âœ… Strong | WebSocket message design, opId for idempotency |
| High-level architecture | âœ… Strong | Write path, read path, presence all covered |
| OT vs. CRDTs deep dive | âœ… Strong | Built from first principles, worked examples |
| WebSocket scaling deep dive | âœ… Strong | Consistent hashing, pub/sub trade-off |
| Compaction deep dive | âœ… Strong | Online vs. offline, versionId CAS mechanism |
| Wrap-up & trade-offs | âœ… Strong | Clear opinions backed by reasoning |
